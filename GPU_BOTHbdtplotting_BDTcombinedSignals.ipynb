{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7eb49f6-aab2-41b5-9d07-80641fcc3fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import partialRegionBDT\n",
    "import pickle\n",
    "import uproot\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import math as mth\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from hist import Hist\n",
    "import hist\n",
    "from scipy import stats\n",
    "import random\n",
    "import jdc \n",
    "from numba import jit, cuda\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73d4d5-9820-4e72-ab3d-18bc1c7314b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Function to split input dataset into xgb DMatrices for test and train. Currently sets test size to 1/7 of train size. Needs to split the event weight and label features from the dataset and input them separately into the DMAtrix. This is done via two different methods - manual removal and the sklearn train_test_split function. Needs to output the weights as well, as train_test_split re-orders the lists and the corresponding weights need to be returned as well as the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ebb345-dd22-46ab-9d9c-483f0f17aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=11, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0caf9d-ea9e-4272-b8a7-786a4348c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestTrain(inputs,features):\n",
    "   x = inputs.loc[:, features].values\n",
    "   y = inputs.loc[:, ['label']].values\n",
    "   masses = inputs.loc[:, ['massPoint']].values\n",
    "   train_input, test_input, train_lbl, test_lbl, train_masses, test_masses = train_test_split(x, y, masses, test_size=1/2.)#, random_state=0)\n",
    "   train_input_withLabel = np.concatenate((train_input, train_lbl), axis=1)  \n",
    "   signalWeights=0\n",
    "   backgroundWeights=0\n",
    "   for x in train_input_withLabel:\n",
    "        if x[len(x)-1] == 1:\n",
    "            signalWeights+=x[len(x)-2]\n",
    "        elif x[len(x)-1] == 0:\n",
    "            backgroundWeights+=x[len(x)-2]\n",
    "   shortfeat = features[0:len(features)-1]\n",
    "    \n",
    "   trainWeights = train_input[:,train_input.shape[1]-1]\n",
    "   testWeights = test_input[:,test_input.shape[1]-1]\n",
    "   train_input = np.delete(train_input,train_input.shape[1]-1,1)\n",
    "   test_input = np.delete(test_input,test_input.shape[1]-1,1)\n",
    "   dtrain = xgb.DMatrix(train_input,label=train_lbl,feature_names=shortfeat,weight=trainWeights)\n",
    "   dtest = xgb.DMatrix(test_input,label=test_lbl,feature_names=shortfeat,weight=testWeights)\n",
    "   return dtrain, dtest, trainWeights, testWeights, backgroundWeights/signalWeights, test_input, test_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfe0aa-f16a-4aa0-97b0-4a4389834fa8",
   "metadata": {},
   "source": [
    "Function that splits data into an array of training features, an array of labels, and an array mass values. Allows the training features to be used independently with the BDT from the labels and mass values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344d571d-cda5-4c5c-b90c-437531dcf46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(inputs,features):\n",
    "   trainingFeatures = inputs.loc[:, features].values\n",
    "   labels = inputs.loc[:, ['label']].values\n",
    "   signalMasses = inputs.loc[:, ['massPoint']].values   \n",
    "   return trainingFeatures, labels, signalMasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ded095-d4d3-41ca-8d81-b2d6552a6a1b",
   "metadata": {},
   "source": [
    "Function that defines the negative mean squared weighted error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dfc715-a802-4c0e-bf69-7dbe70327e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_mean_squared_weighted_error(y_true,y_pred,weights):\n",
    "    weightedSum = 0\n",
    "    totalWeights = 0\n",
    "    for i in range(0, len(y_true)):\n",
    "        weightedSum = weightedSum + weights[i]*(y_true[i] - y_pred[i])**2\n",
    "        totalWeights = totalWeights + weights[i]\n",
    "    \n",
    "    return (-1*weightedSum)/totalWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1755fc-1aae-4cf9-a26e-f4784e2dfa9c",
   "metadata": {},
   "source": [
    "Function that trains different number of BDTs (currently 30), where the BDT parameters are randomly selected and the best \n",
    "set is determined to be the optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b99904-1924-45af-8422-000ac6f4b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def optimize_hyper_xg(inData,variables,njobs):\n",
    "    x,y,masses = format_data(inData,variables)\n",
    "    train_input, test_input, train_lbl, test_lbl, train_masses, test_masses = train_test_split(x, y, masses, test_size=0.2, random_state=0)\n",
    "    train_input_withLabel = np.concatenate((train_input, train_lbl), axis=1)  \n",
    "    signalWeights=0\n",
    "    backgroundWeights=0\n",
    "    for x in train_input_withLabel:\n",
    "        if x[len(x)-1] == 1:\n",
    "            signalWeights+=x[len(x)-2]\n",
    "        elif x[len(x)-1] == 0:\n",
    "            backgroundWeights+=x[len(x)-2]\n",
    "    print(variables)\n",
    "    shortfeat = variables[0:len(variables)-1]\n",
    "    print(shortfeat)\n",
    "    print(len(shortfeat))\n",
    "\n",
    "    trainWeights = train_input[:,train_input.shape[1]-1]\n",
    "    testWeights = test_input[:,test_input.shape[1]-1]\n",
    "    train_input = np.delete(train_input,train_input.shape[1]-1,1)\n",
    "    test_input = np.delete(test_input,test_input.shape[1]-1,1)            \n",
    "    dtrain = xgb.DMatrix(train_input,label=train_lbl,feature_names=shortfeat,weight=trainWeights)\n",
    "    dtest = xgb.DMatrix(test_input,label=test_lbl,feature_names=shortfeat,weight=testWeights)\n",
    "    \n",
    "    trainWeights_scaled = trainWeights.copy()\n",
    "    for idx, label in enumerate(train_lbl):\n",
    "        if label == 1:\n",
    "            trainWeights_scaled[idx] = trainWeights_scaled[idx]*0.0013*(backgroundWeights/signalWeights)\n",
    "    params = {'weights':trainWeights_scaled}\n",
    "    neg_mean_squared_weighted_error_scorer = make_scorer(neg_mean_squared_weighted_error, greater_is_better=True, **params)\n",
    "        \n",
    "    fixedParams = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    "    bdt = xgb.XGBRegressor(objective='binary:logistic',booster='gbtree',eval_metric='logloss',scale_pos_weight=0.0013*(backgroundWeights/signalWeights))\n",
    "    params = {\n",
    "       \"reg_alpha\": stats.uniform(0,2),\n",
    "       \"reg_lambda\": stats.uniform(0,8),\n",
    "       \"min_child_weight\": stats.uniform(0,2),\n",
    "       \"colsample_bytree\": stats.uniform(0.1,0.9),\n",
    "        \"gamma\": stats.uniform(0, 0.3),\n",
    "        \"learning_rate\": stats.uniform(0.2, 1), #stats.uniform(0, 1),\n",
    "        \"max_depth\": stats.randint(2, 6),\n",
    "        \"n_estimators\": stats.randint(60, 400),\n",
    "        \"subsample\": stats.uniform(0.5,0.5),\n",
    "    }\n",
    "    print(SCORERS.keys())\n",
    "#    search = RandomizedSearchCV(bdt, param_distributions=params, scoring = 'neg_mean_squared_error',n_iter=30, cv=5, verbose=1, n_jobs=njobs, return_train_score=False)\n",
    "    search = RandomizedSearchCV(bdt, param_distributions=params, scoring =neg_mean_squared_weighted_error_scorer,n_iter=30, cv=5, verbose=1, n_jobs=njobs, return_train_score=False)\n",
    "    print(len(train_input[:,0:len(variables)-1]))\n",
    "    print(len(variables)-1)\n",
    "    print(search)\n",
    "    out = search.fit(train_input[:,0:len(variables)-1],train_lbl,sample_weight=trainWeights)\n",
    "    print(\"Best parameter set found on development set:\")\n",
    "    print(\"\")\n",
    "    print(out.best_estimator_)\n",
    "    print(out.best_params_)\n",
    "    print(\"BEST roc_auc\")\n",
    "    print(out.best_score_)\n",
    "    return out.best_estimator_, out.best_params_, dtrain, dtest, trainWeights, testWeights, backgroundWeights/signalWeights, train_input, test_input, test_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69dc0c-18bf-4e74-b0fc-c4910fdcdcab",
   "metadata": {},
   "source": [
    "Function that splits the input data into nFolds that are used for training and testing. A number (nFolds) of BDTs are also created and\n",
    "trained using the training data. The function returns 2D arrays of the traing, testing, and BDTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d963aaad-ae81-45e4-b288-9a7fe15c50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kFoldCrossValidation(BDTparams,inData,variables,nFolds,inData_systWeights):\n",
    "    x,y,masses = format_data(inData,variables)\n",
    "    \n",
    "    random.seed()\n",
    "    seed = random.randrange(1, 10000, 1)\n",
    "    seed = 111\n",
    "    \n",
    "    kf = KFold(n_splits=nFolds,shuffle=True,random_state=seed)\n",
    "    array_train_input = []\n",
    "    array_test_input = []\n",
    "    array_test_input_systWeights = []\n",
    "    array_train_lbl = []\n",
    "    array_test_lbl = []\n",
    "    array_train_weight = []\n",
    "    array_test_weight = []\n",
    "    array_masses = []\n",
    "    array_dtrain = []\n",
    "    array_dtest = []\n",
    "    array_bdt =[]\n",
    "    idx=0\n",
    "    print(BDTparams)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        array_train_input.append(x[train_index])\n",
    "        array_test_input.append(x[test_index])\n",
    "        array_test_input_systWeights.append(inData_systWeights.iloc[test_index])\n",
    "        array_train_lbl.append(y[train_index])\n",
    "        array_test_lbl.append(y[test_index])        \n",
    "        array_masses.append(masses[test_index])        \n",
    "        #train_input, test_input, train_lbl, test_lbl = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "        train_input_withLabel = np.concatenate((array_train_input[idx], array_train_lbl[idx]), axis=1)  \n",
    "        signalWeights=0\n",
    "        backgroundWeights=0\n",
    "        for i in train_input_withLabel:\n",
    "            if i[len(i)-1] == 1:\n",
    "                signalWeights+=i[len(i)-2]\n",
    "            elif i[len(i)-1] == 0:\n",
    "                backgroundWeights+=i[len(i)-2]\n",
    "        print(variables)\n",
    "        shortfeat = variables[0:len(variables)-1]\n",
    "        print(shortfeat)\n",
    "        print(len(shortfeat))\n",
    "        BDTparams['scale_pos_weight']=0.0013*(backgroundWeights/signalWeights)\n",
    "\n",
    "        array_train_weight.append(array_train_input[idx][:,array_train_input[idx].shape[1]-1])\n",
    "        array_test_weight.append(array_test_input[idx][:,array_test_input[idx].shape[1]-1])\n",
    "        array_train_input[idx] = np.delete(array_train_input[idx],array_train_input[idx].shape[1]-1,1)\n",
    "        array_test_input[idx] = np.delete(array_test_input[idx],array_test_input[idx].shape[1]-1,1)            \n",
    "        array_dtrain.append(xgb.DMatrix(array_train_input[idx],label=array_train_lbl[idx],feature_names=shortfeat,weight=array_train_weight[idx]))\n",
    "        array_dtest.append(xgb.DMatrix(array_test_input[idx],label=array_test_lbl[idx],feature_names=shortfeat,weight=array_test_weight[idx]))\n",
    "        #evallist = [(dtest,'eval'),(dtrain,'train')]\n",
    "        #bst = xgb.train(BDTparams, dtrain,50,evallist)\n",
    "        array_bdt.append(xgb.XGBRegressor(objective='binary:logistic',booster='gbtree',eval_metric='logloss'))\n",
    "        print(BDTparams)\n",
    "        array_bdt[idx].set_params(**BDTparams)\n",
    "        print(array_bdt[idx].get_params())\n",
    "        print(len(array_test_input[idx][:,0:len(variables)-1]))\n",
    "        print(len(array_test_lbl[idx]))\n",
    "        eval_set = [(array_test_input[idx][:,0:len(variables)-1],array_test_lbl[idx])]\n",
    "        array_bdt[idx].fit(array_train_input[idx][:,0:len(variables)-1],array_train_lbl[idx],sample_weight=array_train_weight[idx],eval_set=eval_set)\n",
    "        idx = idx+1\n",
    "        \n",
    "    return array_bdt, array_dtrain, array_dtest, array_train_input, array_test_input, array_train_lbl, array_test_lbl, array_train_weight, array_test_weight, array_masses, array_test_input_systWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17b7fe-97f1-49ed-b834-7f5e052142a0",
   "metadata": {},
   "source": [
    "Function that combines the various arrays from the kFold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1317d27-0fe6-461a-94af-54cabf5a39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combiningKFolds(aBDT,aTest,aTest_array,aTestWeight,aTestMasses,aTrain,aTrain_array,aTrainWeight,nFolds, systWeights_array):\n",
    "  array_y_predsAltTrain = []\n",
    "  array_labelsaltTrain = []\n",
    "  array_y_predsAlt = []\n",
    "  array_labelsalt = []    \n",
    "  all_y_predsAlt = np.array([])\n",
    "  all_labelsalt = np.array([])\n",
    "  all_systWeights = pd.concat(systWeights_array)\n",
    "  all_aTestWeight = np.array([])\n",
    "  all_aTestMasses = np.concatenate(np.concatenate( aTestMasses, axis=0 ), axis=0)\n",
    "  #allTest_events = np.array([])\n",
    "  allTest_events = []\n",
    "    \n",
    "  for idx in range(0,nFolds):\n",
    "    print(\"Inside combiningKFolds\")\n",
    "    y_predsAlt = aBDT[idx].predict(aTest_array[idx])\n",
    "    all_y_predsAlt = np.concatenate([all_y_predsAlt,y_predsAlt])\n",
    "    all_labelsalt = np.concatenate([all_labelsalt,aTest[idx].get_label()])\n",
    "    all_aTestWeight = np.concatenate([all_aTestWeight,aTestWeight[idx]])\n",
    "    for testEvent in aTest_array[idx]:\n",
    "        allTest_events.append(testEvent)\n",
    "#        allTest_events = np.concatenate([allTest_events,testEvent])\n",
    "    #all_aTestMasses = np.concatenate([all_aTestMasses,aTestMasses[idx]])\n",
    "    #fpralt, tpralt, thresholdsalt = roc_curve(labelsalt, y_predsAlt,sample_weight=aTestWeight[idx])     \n",
    "    \n",
    "    array_y_predsAltTrain.append(aBDT[idx].predict(aTrain_array[idx]))\n",
    "    array_labelsaltTrain.append(aTrain[idx].get_label())\n",
    "    array_y_predsAlt.append(aBDT[idx].predict(aTest_array[idx]))\n",
    "    array_labelsalt.append(aTest[idx].get_label())    \n",
    "    \n",
    "  plotOverTraining_kFold(all_y_predsAlt,all_labelsalt,all_aTestWeight,array_y_predsAltTrain,array_labelsaltTrain,aTrainWeight,\"allVariables\")\n",
    "  plotOverTraining_kFold_individaulTraining(array_y_predsAlt,array_labelsalt,aTestWeight,array_y_predsAltTrain,array_labelsaltTrain,aTrainWeight,\"allVariables\")\n",
    "\n",
    "  print(\"Length of all events used in k-fold\")\n",
    "  print(len(all_y_predsAlt))\n",
    "  print(len(all_labelsalt))\n",
    "  print(len(allTest_events))\n",
    "  print(allTest_events[0])\n",
    "  print(len(all_systWeights))\n",
    "  return all_y_predsAlt, all_labelsalt, all_aTestWeight, all_aTestMasses, allTest_events, all_systWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82c41c-6b9b-4767-adbb-7e6c13296d81",
   "metadata": {},
   "source": [
    "This calculates an array of HE weights for a given depth provided by the input 'sfhist'. The overall weight is found by multiplying the weights for each depth, and takes a while to calculate. Because the bin edges aren't fixed, the correct weight is found by iterating over the bins until the correct one is found rather than using a direct list access. This takes a while to calculate when iterating over every depth in every event.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf506ec-6d10-4042-bc2d-f93b16ad6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeDepthWeight(sfhist,heEnergy):\n",
    "   weights = []\n",
    "   for energyIndex, energy in enumerate(heEnergy):\n",
    "      lastvalue = None\n",
    "      foundbin = False\n",
    "      for idx, value in enumerate(sfhist.values()):\n",
    "         if(sfhist.axis().edges()[idx]>energy):\n",
    "            if idx==1:\n",
    "               heEnergy[energyIndex]=0\n",
    "            weights.append(lastvalue)\n",
    "            foundbin = True\n",
    "            break\n",
    "         lastvalue = value\n",
    "      if not foundbin:\n",
    "         weights.append(lastvalue)\n",
    "   return np.array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a61c8d-40b5-4bcc-a06e-22f8cc8e7d05",
   "metadata": {},
   "source": [
    "Function that calculates CSC reweight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e13f70-a8b7-4de4-bf6e-f38d0013415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcscStationWeight(sfhist,cscSegmentDR):\n",
    "    weights = []\n",
    "    for cscIndex, cscDR in enumerate(cscSegmentDR):\n",
    "      if cscDR > 2:\n",
    "        cscDR = -1      \n",
    "      for idx, value in enumerate(sfhist.axis().edges()):\n",
    "        if(cscDR<value):\n",
    "            cscWeight = sfhist.values()[idx-1]\n",
    "            break\n",
    "      weights.append(cscWeight)\n",
    "    return np.array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca84e79-e35e-4042-aa42-a06d4bcb3312",
   "metadata": {},
   "source": [
    "This function plots all of the training variables. It takes some manual manipulation to change the scale of the plots to make them look nice for every variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b32be2-ebb9-4b66-b29a-84f99289ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "def plotVariables(signalFiles, sigMasses, DYfile, variables):\n",
    "    xlabels = [r'$p_{T, track}$ (GeV)', r'$\\eta_{track}$', r'$\\phi_{track}$', r'$\\Delta R(track, Nearest Standalone Muon)$', \n",
    "               r'$\\Delta \\phi(track, \\mu)*charge_{track}$', r'$E_{\\mu}$', r'$\\chi^{2}_{\\mu}$', r'$\\Delta R(track, Nearest CSC segment)$',\n",
    "               r'disStaChi2', r'disStadPhi', r'disStadEta', r'disStaDEoverE', r'refitStaE', r'refitStaChi2', r'refitStadPhi', r'refitStadEta', r'refitStaDEoverE', ############ new\n",
    "               r'$E_{depth 0}$ (GeV)', r'$E_{depth 1}$ (GeV)', r'$E_{depth 2}$ (GeV)', r'$E_{depth 3}$ (GeV)', \n",
    "               r'$E_{depth 4}$ (GeV)', r'$E_{depth 5}$ (GeV)', r'$E_{depth 6}$ (GeV)', r'$charge_{track}$', r'$\\Delta E(\\mu, track)/E_{track}$',\n",
    "               r'$\\Delta R(track, CSC hit station 0)$', r'$\\Delta R(track, CSC hit station 1)$',\n",
    "               r'$\\Delta R(track, CSC hit station 2)$', r'$\\Delta R(track, CSC hit station 3)$']\n",
    "    \n",
    "    binning = [[0,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,600], \n",
    "              np.arange(-3,3,0.1),np.arange(-3.5,3.5,0.25),np.arange(0,1,0.01),\n",
    "              np.arange(-1,1,0.1), [0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,1000],np.arange(0,15,.25),np.arange(0,.2,.005),\n",
    "              50,np.linspace(-1,1,50),np.linspace(-0.5,0.5,50),np.linspace(-1,1,50),np.linspace(-1,100,50),50,np.linspace(-1,1,50),np.linspace(-0.5,0.5,50),np.linspace(-1,1,50), ########## new\n",
    "              np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),\n",
    "              np.arange(-1.1,1.2,1.1),np.arange(-1,-0.6,0.01),\n",
    "              [-1,-0.5,0,0.025,0.05,0.075,0.1,0.15,0.2,2],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6]]\n",
    "              #[-1,-0.5,0,0.025,0.05,0.075,0.1,0.15,0.2,1,2],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6]]\n",
    "              #np.arange(-1,6,0.1),np.arange(-1,6,0.1),np.arange(-1,6,0.1),np.arange(-1,6,0.1)]\n",
    "\n",
    "#    DYweights_unity = DYfile.loc[lambda DYfile: DYfile['eta']<0,'EventWeight']/DYfile['EventWeight'].sum()\n",
    "    DYweights_unity = DYfile['trainingWeight']/DYfile['trainingWeight'].sum()\n",
    "    for idx1, variable in enumerate(variables):\n",
    "        if idx1 == 30:continue ################################################################################# formerly 21\n",
    "        \n",
    "#        axes = DYfile.loc[lambda DYfile: DYfile['eta']<0,:].hist(column=variable, weights=DYweights_unity, bins=binning[idx1], label='DY', log=True)\n",
    "        axes = DYfile.hist(column=variable, weights=DYweights_unity, bins=binning[idx1], label='DY', log=True)\n",
    "        plt.xlabel(xlabels[idx1], loc='right')\n",
    "        for idx2, signal in enumerate(signalFiles):\n",
    "            if sigMasses[idx2] == '0p4' or sigMasses[idx2] == '0p8':continue\n",
    "#            signalWeights_unity = signal.loc[lambda signal: signal['eta']<0,'EventWeight']/signal['EventWeight'].sum()\n",
    "            signalWeights_unity = signal['trainingWeight']/signal['trainingWeight'].sum()\n",
    "#            signal.loc[lambda signal: signal['eta']<0,:].hist(column=variable, ax=axes, weights=signalWeights_unity, bins=binning[idx1], histtype='step',\n",
    "            signal.hist(column=variable, ax=axes, weights=signalWeights_unity, bins=binning[idx1], histtype='step',\n",
    "                       label=r\"$m_{A'}=$\"+sigMasses[idx2].replace('p','.')+\" GeV\")\n",
    "\n",
    "        plt.ylim([0.004,1])\n",
    "        #plt.ylim([0.001,1])\n",
    "        #plt.ylim([0.0001,1])\n",
    "        #plt.ylim([0,1])        \n",
    "        #plt.ylim([0.0004,1])        \n",
    "        #plt.ylim([0,0.3])        \n",
    "        #plt.ylim([0,0.5])        \n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(False)\n",
    "        plt.title(\" \")\n",
    "        plt.savefig(variable+\".pdf\")\n",
    "        #fig.savefig(variable+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc39de-cbb9-4bdc-91bc-dde5a63052b5",
   "metadata": {},
   "source": [
    "Set the features to pull from the datafile. Can try new trainings by adding/removing features from the lists, but need to make sure that 'EventWeight' is still the last one. Some of the functions are lazy, and assume that the last is event weight when removing it for the BDT training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e7874d-2067-4d54-b1b6-8417ede83caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "   allTrainingFeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"trainingWeight\"]\n",
    "   newTrainingFeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",'disStaChi2','disStadPhi','disStadEta','disStaDEoverE','refitStaE','refitStaChi2','refitStadPhi','refitStadEta','refitStaDEoverE',\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"trainingWeight\"]\n",
    "   allfeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"found_HEDepth_0\",\"found_HEDepth_1\",\"found_HEDepth_2\",\"found_HEDepth_3\",\"found_HEDepth_4\",\"found_HEDepth_5\",\"found_HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"EventWeight\",\"PUupWeight\",\"PUdownWeight\",\"IDupWeight\",\"IDdownWeight\",\"ISOupWeight\",\"ISOdownWeight\",\"TrigUpWeight\",\"TrigDownWeight\",\"EnBinWeight\"]\n",
    "   newfeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",'disStaChi2','disStadPhi','disStadEta','disStaDEoverE','refitStaE','refitStaChi2','refitStadPhi','refitStadEta','refitStaDEoverE',\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"found_HEDepth_0\",\"found_HEDepth_1\",\"found_HEDepth_2\",\"found_HEDepth_3\",\"found_HEDepth_4\",\"found_HEDepth_5\",\"found_HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"EventWeight\",\"PUupWeight\",\"PUdownWeight\",\"IDupWeight\",\"IDdownWeight\",\"ISOupWeight\",\"ISOdownWeight\",\"TrigUpWeight\",\"TrigDownWeight\",\"EnBinWeight\"]\n",
    "   \n",
    "   allTrainingFeatures_noC = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"EventWeight\"]\n",
    "   allTrainingFeaturesNoEvtW = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\"]\n",
    "   allfeatures_noMissingHitBool = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"EventWeight\"]\n",
    "   featuresNoHE = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"EventWeight\"]\n",
    "   systWeightFeatures = [\"standaloneDEoverE\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"EventWeight\",\"PUupWeight\",\"PUdownWeight\",\"IDupWeight\",\"IDdownWeight\",\"ISOupWeight\",\"ISOdownWeight\",\"TrigUpWeight\",\"TrigDownWeight\",\"EnBinWeight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668118b9-381c-4fa6-a0f4-903ca4d60257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self,features,TrainingFeatures):\n",
    "        self.features=features\n",
    "        self.TrainingFeatures=TrainingFeatures\n",
    "all = Features(allfeatures,allTrainingFeatures)\n",
    "new = Features(newfeatures,newTrainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66529743-faa8-4954-a7a3-e51166caf263",
   "metadata": {},
   "source": [
    "Load in the input DY file, and append a list of zeroes as a label to indicate that it is background. Also calls the 'getRocRate' function from partialRegionBDT.py to calculate the false positive rate from the previous cut-based approach for use in plotting on the ROC curves. Additionally, pulls the systematic weights (ID,ISO,trig, etc. up/down) which later get used in making inputs to combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45bf2dcf-8d5d-4829-8284-a0d4076cfc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "   inDir=\"BDT Files/histograms_23_7_5\"\n",
    "   mcFile = uproot.open(\"BDT Files/histograms_23_7_5/ZmmSim_BaseEventFiltered.root\")\n",
    "   falsePos = partialRegionBDT.getRocRate(\"BDT Files/histograms_23_7_5/ZmmSim_BaseEventFiltered.root\")\n",
    "   events=mcFile[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs = events.arrays(newfeatures, library=\"pd\")\n",
    "#mcInputs = mcInputs.drop(mcInputs.columns[1], axis=1)\n",
    "   mcSystWeights = events.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel = []\n",
    "   for entry in range(mcInputs.shape[0]):\n",
    "     mcLabel.append(0)\n",
    "   mcInputs['label'] = mcLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dee8c10-c73f-48d1-993c-d4f89808d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pt       eta       phi     staDR    staPhi        staE  \\\n",
      "0       36.234983  1.680514  0.178885  0.054614  0.048367   46.247898   \n",
      "1       48.588770 -1.953590  2.534030  0.054398  0.052517   59.415211   \n",
      "2       50.687919  2.220909  0.804715  0.068122 -0.068010  115.821770   \n",
      "3       49.187752  1.931597 -1.972739  0.021346 -0.021044  259.733459   \n",
      "4       37.035284 -2.366398  1.975624  0.096683  0.095750   44.757290   \n",
      "...           ...       ...       ...       ...       ...         ...   \n",
      "323010  40.058939  2.391082  1.540583  0.081505  0.059358   93.968254   \n",
      "323011  44.729745  1.914928 -2.060235  0.006756  0.006719  136.491226   \n",
      "323012  64.873260  1.967337 -2.030825  0.030830 -0.029436  123.240128   \n",
      "323013  44.544758 -1.942407  0.027218  0.017032 -0.014497  130.657242   \n",
      "323014  36.115510  2.222723  2.432604  0.146875  0.146773   28.082941   \n",
      "\n",
      "          staChi     cscDR  disStaChi2  disStadPhi  ...  PUupWeight  \\\n",
      "0       7.220243  0.003537    3.989355    0.032591  ...    1.281163   \n",
      "1       0.753268  0.001801    0.740526    0.057978  ...    1.101697   \n",
      "2       1.541431  0.003291    1.426785   -0.318916  ...    1.644816   \n",
      "3       1.857932  0.001489    1.855981   -0.020965  ...    0.914934   \n",
      "4       3.293752  0.005710    3.353067    0.060907  ...    0.988440   \n",
      "...          ...       ...         ...         ...  ...         ...   \n",
      "323010  7.444005  0.007052    0.699285   -0.100440  ...    0.930537   \n",
      "323011  0.548420  0.002684    2.953938    0.046317  ...    0.924770   \n",
      "323012  1.664319  0.002134    2.761692   -0.149842  ...    0.875039   \n",
      "323013  0.604228  0.004076    0.603548   -0.015371  ...    0.949233   \n",
      "323014  2.987962  0.001612    0.791984   -0.765594  ...    0.912289   \n",
      "\n",
      "        PUdownWeight  IDupWeight  IDdownWeight  ISOupWeight  ISOdownWeight  \\\n",
      "0           0.777586    1.066726      1.045268     1.077273       1.034933   \n",
      "1           0.877588    1.033404      1.012724     1.043663       1.002670   \n",
      "2           0.650015    1.121611      1.099166     1.132725       1.088275   \n",
      "3           1.038375    0.988960      0.968769     0.998490       0.959433   \n",
      "4           0.906228    0.983247      0.963464     0.993081       0.953828   \n",
      "...              ...         ...           ...          ...            ...   \n",
      "323010      1.079963    1.013374      0.993095     1.023388       0.983282   \n",
      "323011      1.049539    0.999622      0.979155     1.009255       0.969719   \n",
      "323012      1.053714    0.975275      0.952051     0.983144       0.944378   \n",
      "323013      1.033418    1.010235      0.989551     1.019971       0.980015   \n",
      "323014      1.035374    0.985873      0.966198     0.995733       0.956535   \n",
      "\n",
      "        TrigUpWeight  TrigDownWeight  EnBinWeight  label  \n",
      "0           1.066534        1.045459     1.055997      0  \n",
      "1           1.033273        1.012854     1.023064      0  \n",
      "2           1.121469        1.099308     1.110389      0  \n",
      "3           0.988763        0.968966     0.978864      0  \n",
      "4           0.983394        0.963317     0.973355      0  \n",
      "...              ...             ...          ...    ...  \n",
      "323010      1.013246        0.993223     1.003235      0  \n",
      "323011      0.999241        0.979536     0.989388      0  \n",
      "323012      0.973444        0.953883     0.963663      0  \n",
      "323013      1.009850        0.989936     0.999893      0  \n",
      "323014      0.986013        0.966058     0.976035      0  \n",
      "\n",
      "[323015 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mcInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b78234-7ea4-43f9-b621-bedb6620f3ed",
   "metadata": {},
   "source": [
    "Input for the same sign MC events. Probably not needed anymore, but kept in case test needs to be redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f51cb5-6a34-4e1f-a0ae-d3f2e0c88c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "   mcFile_SameSign = uproot.open(\"BDT Files/CRReReco_SameSign.root\")\n",
    "   events_SameSign=mcFile_SameSign[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs_SameSign = events_SameSign.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   mcSystWeights_SameSign = events_SameSign.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel_SameSign = []\n",
    "   massPoint_SameSign = []\n",
    "   for entry in range(mcInputs_SameSign.shape[0]):\n",
    "     mcLabel_SameSign.append(0)\n",
    "     massPoint_SameSign.append(\"NA\")\n",
    "   mcInputs_SameSign['label'] = mcLabel_SameSign\n",
    "   mcInputs_SameSign['massPoint']=massPoint_SameSign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b60525-431f-4692-aa8b-ef6f25483dd0",
   "metadata": {},
   "source": [
    "Input for the same sign data events. Used to estimate the muPX background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397ad6ea-fd32-4f11-ad98-7c720ea87e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "   dataFile_SameSign = uproot.open(\"BDT Files/sameSignCR_2018Data.root\")\n",
    "   events_data_SameSign=dataFile_SameSign[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   dataInputs_SameSign = events_data_SameSign.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   dataLabel_SameSign = []\n",
    "   massPoint_SameSign = []\n",
    "   for entry in range(dataInputs_SameSign.shape[0]):\n",
    "     dataLabel_SameSign.append(0)\n",
    "     massPoint_SameSign.append(\"NA\")\n",
    "   dataInputs_SameSign['label'] = dataLabel_SameSign\n",
    "   dataInputs_SameSign['massPoint']=massPoint_SameSign   \n",
    "   dataInputs_SameSign['staPhi']=dataInputs_SameSign['staPhi']*dataInputs_SameSign['probeCharge']\n",
    "   dataInputs_SameSign['trainingWeight'] = dataInputs_SameSign['EventWeight']    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547cbd1-0975-4163-9a90-b49533dc024a",
   "metadata": {},
   "source": [
    "Input the high HCAL events, both data and MC. Probably not needed anymore but kept in case the data vs MC check needs to be redone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10548b0-aa39-49d0-921e-273b688869d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   mcFile_highHCAL = uproot.open(\"BDT Files/highHcal_mc.root\")\n",
    "#   mcFile_highHCAL = uproot.open(\"/data/cmszfs1/user/krohn045/DarkPhoton/MichaelsCode/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/HighHCAL_CR/DYJets/hists/highHcal_mc.root\")\n",
    "   events_highHCAL=mcFile_highHCAL[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs_highHCAL = events_highHCAL.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   mcSystWeights_highHCAL = events_highHCAL.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel_highHCAL = []\n",
    "   massPoint_highHCAL = []\n",
    "   for entry in range(mcInputs_highHCAL.shape[0]):\n",
    "     mcLabel_highHCAL.append(0)\n",
    "     massPoint_highHCAL.append(\"NA\")\n",
    "   mcInputs_highHCAL['label'] = mcLabel_highHCAL\n",
    "   mcInputs_highHCAL['massPoint']=massPoint_highHCAL\n",
    "    \n",
    "   dataFile_highHCAL = uproot.open(\"BDT Files/highHcal_data.root\")\n",
    "#   dataFile_highHCAL = uproot.open(\"/data/cmszfs1/user/krohn045/DarkPhoton/MichaelsCode/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/HighHCAL_CR/Data/hists/highHcal_data.root\")\n",
    "   events_highHCAL=dataFile_highHCAL[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   dataInputs_highHCAL = events_highHCAL.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   dataSystWeights_highHCAL = events_highHCAL.arrays(systWeightFeatures, library=\"pd\")\n",
    "   dataLabel_highHCAL = []\n",
    "   massPoint_highHCAL = []\n",
    "   for entry in range(dataInputs_highHCAL.shape[0]):\n",
    "     dataLabel_highHCAL.append(0)\n",
    "     massPoint_highHCAL.append(\"NA\")\n",
    "   dataInputs_highHCAL['label'] = dataLabel_highHCAL\n",
    "   dataInputs_highHCAL['massPoint']=massPoint_highHCAL    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce06645-4076-4bdb-aa3a-e474d9a34a53",
   "metadata": {},
   "source": [
    "Function to merge the signal files, while keeping the weights the same across each signal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "552e8a98-3a99-4797-9179-8baf7240c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSignalFiles(signalArray):\n",
    "    maxWeight = 0\n",
    "    for signal in signalArray:\n",
    "        if maxWeight < signal['EventWeight'].sum():\n",
    "            maxWeight = signal['EventWeight'].sum()\n",
    "    for signal in signalArray:\n",
    "        signal['trainingWeight'] = (maxWeight/signal['EventWeight'].sum())*signal['EventWeight']\n",
    "        print(\"CHECKING REWEIGHTING OF SAMPLES\")\n",
    "        print(signal['EventWeight'].sum())\n",
    "        print(len(signal['EventWeight']))\n",
    "    return pd.concat([signalArray[0],signalArray[1],signalArray[2],signalArray[3],signalArray[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964376d-7381-43cf-96b2-adadb6c64e90",
   "metadata": {},
   "source": [
    "Load in the signal files. Assumes a certain filename structure (contains \"DBrem\" and has the mass just before .root). Gets the true positive rate and systematic weights in the same way as the DY loading, and labels them all with 1 to indicate signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "895947a7-9930-4140-ab79-3fc446198110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0p2\n",
      "0p2\n",
      "0p4\n",
      "0p4\n",
      "0p6\n",
      "0p6\n",
      "0p8\n",
      "0p8\n",
      "1p0\n",
      "1p0\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "18267.07831790964\n",
      "18845\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "8570.154183906869\n",
      "8837\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "4307.882830158529\n",
      "4440\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "3063.4657461282304\n",
      "3155\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2569.878775820811\n",
      "2651\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "19872.288698443277\n",
      "20487\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "9176.925738220052\n",
      "9462\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "4083.039590230562\n",
      "4204\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2919.3239197365365\n",
      "3008\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2125.410242738193\n",
      "2186\n"
     ]
    }
   ],
   "source": [
    "   sigFiles = []\n",
    "   sigFiles_ecalBrem = []\n",
    "   sigSystWeights = []\n",
    "   sigMasses = []\n",
    "   sigMasses_ecalBrem = []\n",
    "   sigSystWeights_ecalBrem = []    \n",
    "   truePos = []\n",
    "   scaleFactor = {'0p2':800,'0p4':1200,'0p6':1600,'0p8':2500,'1p0':3200}\n",
    "   #{\"0p2\":4000,\"1p0\":40000,\"0p4\":8000,\"0p8\":16000,\"0p6\":12000,\"2p0\":80000}\n",
    "   for filename in os.listdir(inDir):\n",
    "      f = os.path.join(inDir, filename)\n",
    "      if os.path.isfile(f):\n",
    "          if \"DBrem_map\" in filename and \"fixBiasing\" in filename:\n",
    "             apmass = filename.split(\"_\")[1]\n",
    "             for mass, sf in scaleFactor.items():\n",
    "               if mass in apmass:\n",
    "                 print(mass)\n",
    "                 sigMasses.append(mass)\n",
    "                 sigEvents=uproot.open(str(inDir)+\"/\"+filename+\":demo/partialDisappear/sigVariables\")\n",
    "                 sigFiles.append(sigEvents.arrays(newfeatures, library=\"pd\"))\n",
    "                 sigSystWeights.append(sigEvents.arrays(systWeightFeatures,library=\"pd\"))\n",
    "                 truePos.append(partialRegionBDT.getRocRate(str(inDir)+\"/\"+filename))\n",
    "                 massLabel=[]\n",
    "                 massPoint = []\n",
    "                 for entry in range(sigFiles[-1].shape[0]):\n",
    "                   massLabel.append(1)\n",
    "                   massPoint.append(mass)\n",
    "                 sigFiles[-1]['label']=massLabel\n",
    "                 sigFiles[-1]['massPoint'] = massPoint\n",
    "          elif \"DBrem_map\" in filename and \"ecalBrem\" in filename:\n",
    "             apmass = filename.split(\"_\")[1]\n",
    "             for mass, sf in scaleFactor.items():\n",
    "               if mass in apmass:\n",
    "                 print(mass)\n",
    "                 sigMasses_ecalBrem.append(mass)\n",
    "                 sigEvents=uproot.open(str(inDir)+\"/\"+filename+\":demo/partialDisappear/sigVariables\")\n",
    "                 sigFiles_ecalBrem.append(sigEvents.arrays(newfeatures, library=\"pd\"))\n",
    "                 sigSystWeights_ecalBrem.append(sigEvents.arrays(systWeightFeatures,library=\"pd\"))\n",
    "                 truePos.append(partialRegionBDT.getRocRate(str(inDir)+\"/\"+filename))\n",
    "                 massLabel=[]\n",
    "                 massPoint = []\n",
    "                 for entry in range(sigFiles_ecalBrem[-1].shape[0]):\n",
    "                   massLabel.append(1)\n",
    "                   massPoint.append(mass)\n",
    "                 sigFiles_ecalBrem[-1]['label']=massLabel\n",
    "                 sigFiles_ecalBrem[-1]['massPoint'] = massPoint                    \n",
    "   allSignals = mergeSignalFiles([sigFiles[0],sigFiles[1],sigFiles[2],sigFiles[3],sigFiles[4]])\n",
    "   allSignals_ecalBrem = mergeSignalFiles([sigFiles_ecalBrem[0],sigFiles_ecalBrem[1],sigFiles_ecalBrem[2],sigFiles_ecalBrem[3],sigFiles_ecalBrem[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b154d4-ca69-483d-9a05-df496cb8247a",
   "metadata": {},
   "source": [
    "My previous BDT training was done with the initial ~100,000 privately generated endcap muon samples, and now we have an additional ~130,000 endcap muons available. This part loads in the new files as a separate dataframe so that I can test how the BDT performs on completely independent data. In the future we should probably combine these with the original files. TO DO: Combine inputs using these files need a slightly different cross section scaling because of the gen-level filter requiring a muon in the endcap, so I need to run gen xsec analyzer (or manually find the gen matching efficiency) and include it in the total cross section scaling to get the correct expected number of events.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16789b2a-63e4-44ee-995f-fbf6a4db7fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323015\n"
     ]
    }
   ],
   "source": [
    "newGenMCFile = uproot.open(\"BDT Files/ZmmSim_BaseEventFiltered.root\")\n",
    "newGenfalsePos = partialRegionBDT.getRocRate(\"BDT Files/ZmmSim_BaseEventFiltered.root\")\n",
    "newGenEvents=newGenMCFile[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "newGenMCInputs = newGenEvents.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "newGenMCSystWeights = newGenEvents.arrays(systWeightFeatures, library=\"pd\")\n",
    "print(len(newGenMCSystWeights))\n",
    "newGenMCLabel = []\n",
    "massPoint = []\n",
    "for entry in range(newGenMCInputs.shape[0]):\n",
    "  newGenMCLabel.append(0)\n",
    "  massPoint.append(\"NA\")\n",
    "newGenMCInputs['label']=newGenMCLabel\n",
    "newGenMCInputs['massPoint']=massPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e70e5-0d8d-42e6-8f20-9b3adb846abe",
   "metadata": {},
   "source": [
    "This part calculates and applies the hcal depth-by-depth scale factors. The on-edge and off-edge splitting hasn't been added to the analyzer used to make BDT inputs yet, so these scale factors can be fairly large. The scale factor histograms are the ratio of data events in a bin of HE energy over MC events in that bin, so the total event weight is found by multiplying together these ratios across every depth. getHeDepthWeight calculates the array of weights for all events, so the overall event weight is found as the product of all six arrays. To make sure that the weight isn't applied multiple times as the sheet gets used, create a copy of the base one before applying the weight to it each time.\n",
    "Lastly, I forgot to multiply the standalone delta phi by the probe charge when I made the input trees, so I do it here. Included in this block to make sure that it can't get run twice without reloading and accidentally undo the flip.\n",
    "\n",
    "**To Check** am I putting a zero or -1 in when the event does not have an expected hit at a particular depth? Did I make sure the weight is one for those depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79f58a1d-36a3-4328-8afb-1610b742c5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"BDT Files/weightedMcInputs.pkl\" \n",
    "\n",
    "inputs = []\n",
    "hcalSfHists = []\n",
    "for depth in range(7):\n",
    "    hcalSfHists.append(uproot.open(\"BDT Files/hcalSFs.root:hcalSF_depth\"+str(depth)))\n",
    "\n",
    "if os.path.exists(file_path):  \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        weightedMcInputs = pickle.load(file)\n",
    "else:\n",
    "    weightedMcInputs = mcInputs.copy()\n",
    "    reweight=getHeDepthWeight(hcalSfHists[0],weightedMcInputs['HEDepth_'+str(0)])\n",
    "    for depth in range(1,7):\n",
    "        reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMcInputs['HEDepth_'+str(depth)])\n",
    "    weightedMcInputs['EventWeight']=weightedMcInputs['EventWeight']*reweight  \n",
    "    weightedMcInputs['trainingWeight'] = weightedMcInputs['EventWeight']\n",
    "    weightedMcInputs['PUupWeight']=weightedMcInputs['PUupWeight']*reweight  \n",
    "    weightedMcInputs['PUdownWeight']=weightedMcInputs['PUdownWeight']*reweight  \n",
    "    weightedMcInputs['IDupWeight']=weightedMcInputs['IDupWeight']*reweight  \n",
    "    weightedMcInputs['IDdownWeight']=weightedMcInputs['IDdownWeight']*reweight  \n",
    "    weightedMcInputs['ISOupWeight']=weightedMcInputs['ISOupWeight']*reweight  \n",
    "    weightedMcInputs['ISOdownWeight']=weightedMcInputs['ISOdownWeight']*reweight  \n",
    "    weightedMcInputs['TrigUpWeight']=weightedMcInputs['TrigUpWeight']*reweight  \n",
    "    weightedMcInputs['TrigDownWeight']=weightedMcInputs['TrigDownWeight']*reweight  \n",
    "    weightedMcInputs['EnBinWeight']=weightedMcInputs['EnBinWeight']*reweight  \n",
    "\n",
    "    # save data to file\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(weightedMcInputs, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a232a8-ac40-41bf-a5de-6c7d036f06f4",
   "metadata": {},
   "source": [
    "Calculates the CSC reweighting values and creates a new event weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f34cc0b-c4cd-4035-bd25-86f2e44b2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[-1.   -0.97 -0.94 -0.91 -0.88 -0.85 -0.82 -0.79 -0.76 -0.73 -0.7  -0.67\n",
      " -0.64 -0.61 -0.58 -0.55 -0.52 -0.49 -0.46 -0.43 -0.4  -0.37 -0.34 -0.31\n",
      " -0.28 -0.25 -0.22 -0.19 -0.16 -0.13 -0.1  -0.07 -0.04 -0.01  0.02  0.17\n",
      "  1.97]\n",
      "[6.053152   0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.97274566 0.86921054 1.3505379 ]\n"
     ]
    }
   ],
   "source": [
    "cscSfHists = []\n",
    "for depth in range(4):\n",
    "    print(depth)\n",
    "    cscSfHists.append(uproot.open(\"BDT Files/cscStationSFs.root:cscScaleFactor_station\"+str(depth)))\n",
    "print(cscSfHists[0].axis().edges())\n",
    "print(cscSfHists[0].values())\n",
    "cscReweight0=getcscStationWeight(cscSfHists[0],weightedMcInputs['cscDRbyStation_'+str(0)])\n",
    "cscReweight1=getcscStationWeight(cscSfHists[1],weightedMcInputs['cscDRbyStation_'+str(1)])\n",
    "cscReweight2=getcscStationWeight(cscSfHists[2],weightedMcInputs['cscDRbyStation_'+str(2)])\n",
    "cscReweight3=getcscStationWeight(cscSfHists[3],weightedMcInputs['cscDRbyStation_'+str(3)])\n",
    "weightedMcInputs['EventWeight_cscReweight']=weightedMcInputs['EventWeight']*cscReweight0*cscReweight1*cscReweight2*cscReweight3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563c736-e408-46d6-aeec-deb99c12f9a6",
   "metadata": {},
   "source": [
    "For all of the various samples, set HE depth energy values when hits don't exist to -1. So that the BDT distinguishes from hits with 0 energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "458252fa-f891-4e2e-a3e9-0825d1c3260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pt       eta       phi     staDR    staPhi        staE    staChi  \\\n",
      "0     40.688169  1.829130  2.881959  0.013875 -0.000432  122.879028  0.301895   \n",
      "1     43.054313  1.746340 -0.262863  0.011356  0.011161  147.616638  0.960420   \n",
      "2     40.569966  2.202706  1.433373  0.004546  0.002755  218.705139  0.627256   \n",
      "3     31.126480 -2.079570 -2.041045  0.163064 -0.052736   13.001960  1.432543   \n",
      "4     66.612570  1.838144  0.626287  0.051073 -0.051048   73.318413  0.628479   \n",
      "...         ...       ...       ...       ...       ...         ...       ...   \n",
      "2646  25.956697  1.679126  2.973396  0.044959 -0.040870   49.544533  1.280769   \n",
      "2647  46.760750 -1.726106  1.106414  0.096718 -0.096210   43.609550  1.489939   \n",
      "2648  47.668459 -1.927518 -0.253790  0.010157 -0.010152  136.373871  0.565698   \n",
      "2649  43.922674  2.088944  2.485391  0.627433 -0.627275    7.145236  0.686833   \n",
      "2650  30.699898 -2.082791  2.693759  0.016342 -0.016335  183.138077  0.521087   \n",
      "\n",
      "         cscDR  disStaChi2  disStadPhi  ...  IDdownWeight  ISOupWeight  \\\n",
      "0     0.000858    0.301877   -0.000499  ...      0.964535     0.994186   \n",
      "1     0.004398    0.977843    0.009886  ...      0.951866     0.981066   \n",
      "2     0.003335    0.627187    0.002709  ...      0.930533     0.961275   \n",
      "3     0.121063    1.400460   -0.050280  ...      1.040227     1.072113   \n",
      "4     0.001246    0.619324   -0.054272  ...      0.937304     0.966058   \n",
      "...        ...         ...         ...  ...           ...          ...   \n",
      "2646  0.009030    1.247381   -0.133403  ...      1.002299     1.033748   \n",
      "2647  0.003230    1.371207   -0.129013  ...      1.006906     1.037653   \n",
      "2648  0.001793    0.566054   -0.010258  ...      0.963541     0.993162   \n",
      "2649  0.024621    0.704607   -0.666378  ...      0.978059     1.007925   \n",
      "2650  0.002016    2.014431    0.433218  ...      0.985466     1.015579   \n",
      "\n",
      "      ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  label  \\\n",
      "0          0.955241      0.984321        0.964911     1.007736      1   \n",
      "1          0.942420      0.971526        0.951766     0.994325      1   \n",
      "2          0.918987      0.949690        0.930337     0.971958      1   \n",
      "3          1.029736      1.061560        1.040075     1.086527      1   \n",
      "4          0.928274      0.956648        0.937495     0.958557      1   \n",
      "...             ...           ...             ...          ...    ...   \n",
      "2646       0.991691      1.022785        1.002437     1.042172      1   \n",
      "2647       0.996915      1.027304        1.007061     1.082726      1   \n",
      "2648       0.954256      0.983307        0.963917     0.979962      1   \n",
      "2649       0.968354      0.997872        0.978209     0.994485      1   \n",
      "2650       0.975635      1.005668        0.985345     1.044287      1   \n",
      "\n",
      "      massPoint  trainingWeight  EventWeight_cscReweight  \n",
      "0           0p2        0.974616                 0.974616  \n",
      "1           0p2        0.961646                 0.961646  \n",
      "2           0p2        0.940014                 0.940014  \n",
      "3           0p2        1.050818                 1.050818  \n",
      "4           0p2        0.947072                 0.947072  \n",
      "...         ...             ...                      ...  \n",
      "2646        1p0        7.197788                 1.012611  \n",
      "2647        1p0        7.230280                 1.017182  \n",
      "2648        1p0        6.920577                 0.973612  \n",
      "2649        1p0        7.023137                 0.988040  \n",
      "2650        1p0        7.076209                 0.995507  \n",
      "\n",
      "[37928 rows x 53 columns]\n",
      "[             pt       eta       phi     staDR    staPhi        staE    staChi  \\\n",
      "0     36.234983  1.680514  0.178885  0.054614  0.048367   46.247898  7.220243   \n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517   59.415211  0.753268   \n",
      "2     50.687919  2.220909  0.804715  0.068122 -0.068010  115.821770  1.541431   \n",
      "3     49.187752  1.931597 -1.972739  0.021346 -0.021044  259.733459  1.857932   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750   44.757290  3.293752   \n",
      "...         ...       ...       ...       ...       ...         ...       ...   \n",
      "2646  25.956697  1.679126  2.973396  0.044959 -0.040870   49.544533  1.280769   \n",
      "2647  46.760750 -1.726106  1.106414  0.096718 -0.096210   43.609550  1.489939   \n",
      "2648  47.668459 -1.927518 -0.253790  0.010157 -0.010152  136.373871  0.565698   \n",
      "2649  43.922674  2.088944  2.485391  0.627433 -0.627275    7.145236  0.686833   \n",
      "2650  30.699898 -2.082791  2.693759  0.016342 -0.016335  183.138077  0.521087   \n",
      "\n",
      "         cscDR  disStaChi2  disStadPhi  ...  IDdownWeight  ISOupWeight  \\\n",
      "0     0.003537    3.989355    0.032591  ...      1.563022     1.610880   \n",
      "1     0.001801    0.740526    0.057978  ...      1.858269     1.915039   \n",
      "2     0.003291    1.426785   -0.318916  ...      1.132262     1.166831   \n",
      "3     0.001489    1.855981   -0.020965  ...      1.126282     1.160835   \n",
      "4     0.005710    3.353067    0.060907  ...      1.031290     1.062992   \n",
      "...        ...         ...         ...  ...           ...          ...   \n",
      "2646  0.009030    1.247381   -0.133403  ...      1.002299     1.033748   \n",
      "2647  0.003230    1.371207   -0.129013  ...      1.006906     1.037653   \n",
      "2648  0.001793    0.566054   -0.010258  ...      0.963541     0.993162   \n",
      "2649  0.024621    0.704607   -0.666378  ...      0.978059     1.007925   \n",
      "2650  0.002016    2.014431    0.433218  ...      0.985466     1.015579   \n",
      "\n",
      "      ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  label  \\\n",
      "0          1.547568      1.594822        1.563308     1.579065      0   \n",
      "1          1.839821      1.895975        1.858508     1.877242      0   \n",
      "2          1.121043      1.155236        1.132408     1.143822      0   \n",
      "3          1.115428      1.149526        1.126511     1.138018      0   \n",
      "4          1.020975      1.052623        1.031133     1.041878      0   \n",
      "...             ...           ...             ...          ...    ...   \n",
      "2646       0.991691      1.022785        1.002437     1.042172      1   \n",
      "2647       0.996915      1.027304        1.007061     1.082726      1   \n",
      "2648       0.954256      0.983307        0.963917     0.979962      1   \n",
      "2649       0.968354      0.997872        0.978209     0.994485      1   \n",
      "2650       0.975635      1.005668        0.985345     1.044287      1   \n",
      "\n",
      "      trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0           1.579065                 1.259129        NaN  \n",
      "1           1.877242                 1.745842        NaN  \n",
      "2           1.143822                 1.063759        NaN  \n",
      "3           1.138018                 1.058362        NaN  \n",
      "4           1.041878                 0.865819        NaN  \n",
      "...              ...                      ...        ...  \n",
      "2646        7.197788                 1.012611        1p0  \n",
      "2647        7.230280                 1.017182        1p0  \n",
      "2648        6.920577                 0.973612        1p0  \n",
      "2649        7.023137                 0.988040        1p0  \n",
      "2650        7.076209                 0.995507        1p0  \n",
      "\n",
      "[360943 rows x 53 columns]]\n",
      "CHECKING WEIGHT VALUES\n",
      "             pt       eta       phi     staDR    staPhi        staE    staChi  \\\n",
      "0     33.490278  1.717840 -2.970404  0.005591 -0.005388   91.913979  0.400194   \n",
      "1     45.611144  2.120171 -1.608827  0.092025  0.091958   38.020702  1.457658   \n",
      "2     57.949479 -1.946880  1.466549  0.238449  0.232412   31.830992  0.463997   \n",
      "3     48.926747  2.264881 -1.678836  0.287264  0.287261   15.313004  2.503192   \n",
      "4     31.692128  1.646233  2.919638  0.247288  0.233991   14.703446  1.055910   \n",
      "...         ...       ...       ...       ...       ...         ...       ...   \n",
      "3150  42.043259  1.979610  1.894294  0.008176  0.007181  171.542648  0.771156   \n",
      "3151  46.627070 -1.831358  2.602722  0.006609  0.006402  134.220779  0.479760   \n",
      "3152  25.441795  1.553023  0.558573  0.139897 -0.133827   24.163887  1.256369   \n",
      "3153  35.071095  1.888268  0.813350  0.216143 -0.216014   18.459581  3.552273   \n",
      "3154  45.588241  2.267202 -2.868251  0.499868  0.489192   15.006203  1.886375   \n",
      "\n",
      "         cscDR  disStaChi2  disStadPhi  ...  IDdownWeight  ISOupWeight  \\\n",
      "0     0.004199    0.401830   -0.003615  ...      0.949436     0.978540   \n",
      "1     0.004749    1.634376    0.080074  ...      1.048798     1.080853   \n",
      "2     0.019189    0.462179    0.234786  ...      1.042546     1.074410   \n",
      "3     0.007731    2.325550    0.382817  ...      0.864218     0.890692   \n",
      "4     0.037670    0.994198    0.253465  ...      0.947598     0.976645   \n",
      "...        ...         ...         ...  ...           ...          ...   \n",
      "3150  0.001267    0.773990    0.008003  ...      1.002647     1.033264   \n",
      "3151  0.003422    0.467791    0.013133  ...      1.002473     1.033085   \n",
      "3152  0.010429    0.944599    0.533522  ...      0.959964     0.990016   \n",
      "3153  0.017465    3.994626   -0.580957  ...      0.951911     0.981065   \n",
      "3154  0.015367    3.134449    0.229066  ...      0.969153     0.998702   \n",
      "\n",
      "      ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  label  \\\n",
      "0          0.939861      0.968908        0.949298     0.949989      1   \n",
      "1          1.038485      1.070136        1.048990     1.049495      1   \n",
      "2          1.032182      1.063700        1.042680     1.059043      1   \n",
      "3          0.855929      0.882054        0.864394     0.882650      1   \n",
      "4          0.938041      0.967031        0.947460     0.987964      1   \n",
      "...             ...           ...             ...          ...    ...   \n",
      "3150       0.992698      1.022958        1.002801     0.994714      1   \n",
      "3151       0.992526      1.022781        1.002627     1.006386      1   \n",
      "3152       0.949682      0.979643        0.959845     0.982422      1   \n",
      "3153       0.942536      0.971366        0.952042     0.944596      1   \n",
      "3154       0.959580      0.988785        0.969301     0.947952      1   \n",
      "\n",
      "      trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0           5.719016                 0.959103        0p8  \n",
      "1           6.318047                 1.059563        0p8  \n",
      "2           6.280047                 1.053190        0p8  \n",
      "3           5.206932                 0.873224        0p8  \n",
      "4           5.707941                 0.957246        0p8  \n",
      "...              ...                      ...        ...  \n",
      "3150        6.039678                 1.012879        0p8  \n",
      "3151        6.038631                 1.012704        0p8  \n",
      "3152        5.782468                 0.969744        0p8  \n",
      "3153        5.734524                 0.961704        0p8  \n",
      "3154        5.837918                 0.979043        0p8  \n",
      "\n",
      "[3155 rows x 53 columns]\n",
      "0       0.959103\n",
      "1       1.059563\n",
      "2       1.053190\n",
      "3       0.873224\n",
      "4       0.957246\n",
      "          ...   \n",
      "3150    1.012879\n",
      "3151    1.012704\n",
      "3152    0.969744\n",
      "3153    0.961704\n",
      "3154    0.979043\n",
      "Name: EventWeight, Length: 3155, dtype: float64\n",
      "1767     0.0\n",
      "5818     0.0\n",
      "7611     0.0\n",
      "10759    0.0\n",
      "11455    0.0\n",
      "        ... \n",
      "1685     0.0\n",
      "2021     0.0\n",
      "2922     0.0\n",
      "3006     0.0\n",
      "180      0.0\n",
      "Name: HEDepth_0, Length: 190, dtype: float64\n",
      "1767    -1.0\n",
      "5818    -1.0\n",
      "7611    -1.0\n",
      "10759   -1.0\n",
      "11455   -1.0\n",
      "        ... \n",
      "1685    -1.0\n",
      "2021    -1.0\n",
      "2922    -1.0\n",
      "3006    -1.0\n",
      "180     -1.0\n",
      "Name: HEDepth_0, Length: 190, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "weightedInputs = []\n",
    "weightedInputsIndividualSignals = []\n",
    "for sigInput in sigFiles:\n",
    "   inputs.append(pd.concat([mcInputs,sigInput]))\n",
    "   weightedInputsIndividualSignals.append(pd.concat([weightedMcInputs,sigInput]))\n",
    "allSignals['EventWeight_cscReweight']=allSignals['EventWeight']\n",
    "allSignals_ecalBrem['EventWeight_cscReweight']=allSignals_ecalBrem['EventWeight']\n",
    "print(allSignals)\n",
    "weightedInputs.append(pd.concat([weightedMcInputs,allSignals]))\n",
    "print(weightedInputs)\n",
    "for dataset in weightedInputs:\n",
    "   print(\"CHECKING WEIGHT VALUES\")\n",
    "   print(dataset[dataset['massPoint']=='0p8'])\n",
    "   print(dataset[dataset['massPoint']=='0p8']['EventWeight'])\n",
    "for dataset in weightedInputs:\n",
    "   dataset['staPhi']=dataset['staPhi']*dataset['probeCharge']\n",
    "   print(dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0']) \n",
    "   dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0'] = dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_1']==0,'HEDepth_1'] = dataset.loc[dataset['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_2']==0,'HEDepth_2'] = dataset.loc[dataset['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_3']==0,'HEDepth_3'] = dataset.loc[dataset['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_4']==0,'HEDepth_4'] = dataset.loc[dataset['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_5']==0,'HEDepth_5'] = dataset.loc[dataset['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_6']==0,'HEDepth_6'] = dataset.loc[dataset['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)\n",
    "   print(dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0']) \n",
    "for sigFile in sigFiles:\n",
    "   sigFile['staPhi']=sigFile['staPhi']*sigFile['probeCharge']\n",
    "   sigFile.loc[sigFile['found_HEDepth_0']==0,'HEDepth_0'] = sigFile.loc[sigFile['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_1']==0,'HEDepth_1'] = sigFile.loc[sigFile['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_2']==0,'HEDepth_2'] = sigFile.loc[sigFile['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_3']==0,'HEDepth_3'] = sigFile.loc[sigFile['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_4']==0,'HEDepth_4'] = sigFile.loc[sigFile['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_5']==0,'HEDepth_5'] = sigFile.loc[sigFile['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_6']==0,'HEDepth_6'] = sigFile.loc[sigFile['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)\n",
    "allSignals_ecalBrem['staPhi']=allSignals_ecalBrem['staPhi']*allSignals_ecalBrem['probeCharge']\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_0']==0,'HEDepth_0'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_1']==0,'HEDepth_1'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_2']==0,'HEDepth_2'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_3']==0,'HEDepth_3'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_4']==0,'HEDepth_4'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_5']==0,'HEDepth_5'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_6']==0,'HEDepth_6'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)    \n",
    "weightedMcInputs['staPhi']=weightedMcInputs['staPhi']*weightedMcInputs['probeCharge']\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_0']==0,'HEDepth_0'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_1']==0,'HEDepth_1'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_2']==0,'HEDepth_2'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_3']==0,'HEDepth_3'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_4']==0,'HEDepth_4'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_5']==0,'HEDepth_5'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_6']==0,'HEDepth_6'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f8c783-35ec-41ba-ad8d-9eb21d381ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedMcInputs_SameSign = mcInputs_SameSign.copy()\n",
    "reweight=getHeDepthWeight(hcalSfHists[0],weightedMcInputs_SameSign['HEDepth_'+str(0)])\n",
    "for depth in range(1,7):\n",
    "   reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMcInputs_SameSign['HEDepth_'+str(depth)])\n",
    "weightedMcInputs_SameSign['EventWeight']=weightedMcInputs_SameSign['EventWeight']*reweight\n",
    "weightedMcInputs_SameSign['staPhi']=weightedMcInputs_SameSign['staPhi']*weightedMcInputs_SameSign['probeCharge']\n",
    "weightedMcInputs_SameSign['trainingWeight'] = weightedMcInputs_SameSign['EventWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ef40d7e-a253-4489-9223-dcfcbe40e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedMCInputs_highHCAL = mcInputs_highHCAL.copy()\n",
    "reweight=getHeDepthWeight(hcalSfHists[0],weightedMCInputs_highHCAL['HEDepth_'+str(0)])\n",
    "for depth in range(1,7):\n",
    "   reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMCInputs_highHCAL['HEDepth_'+str(depth)])\n",
    "weightedMCInputs_highHCAL['EventWeight']=weightedMCInputs_highHCAL['EventWeight']*reweight\n",
    "weightedMCInputs_highHCAL['staPhi']=weightedMCInputs_highHCAL['staPhi']*weightedMCInputs_highHCAL['probeCharge']\n",
    "weightedMCInputs_highHCAL['trainingWeight'] = weightedMCInputs_highHCAL['EventWeight']\n",
    "\n",
    "dataInputs_highHCAL['staPhi']=dataInputs_highHCAL['staPhi']*dataInputs_highHCAL['probeCharge']\n",
    "dataInputs_highHCAL['trainingWeight'] = dataInputs_highHCAL['EventWeight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0665b-d776-4260-a830-5f8537735f68",
   "metadata": {},
   "source": [
    "This plots the DY and signal shapes of the various training variables prior to any BDT selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24c70956-6834-4758-a243-b1724517881c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacut = -0.6\n",
    "DYforPlotting = weightedMcInputs[(weightedMcInputs['standaloneDEoverE']<stacut) & (weightedMcInputs['cellEdgeDeta']>0.004) & (weightedMcInputs['cellEdgeDphi']>0.016)]# & ((weightedMcInputs['HEDepth_0']==0) | (weightedMcInputs['HEDepth_1']==0) | (weightedMcInputs['HEDepth_2']==0) | (weightedMcInputs['HEDepth_3']==0) | (weightedMcInputs['HEDepth_4']==0) | (weightedMcInputs['HEDepth_5']==0) | (weightedMcInputs['HEDepth_6']==0))]\n",
    "\n",
    "sigFilesForPlotting = []\n",
    "for sigFile in sigFiles:\n",
    "    signalForPlotting = sigFile[(sigFile['standaloneDEoverE']<stacut) & (sigFile['cellEdgeDeta']>0.004) & (sigFile['cellEdgeDphi']>0.016)]# & ((sigFile['HEDepth_0']==0) | (sigFile['HEDepth_1']==0) | (sigFile['HEDepth_2']==0) | (sigFile['HEDepth_3']==0) | (sigFile['HEDepth_4']==0) | (sigFile['HEDepth_5']==0) | (sigFile['HEDepth_6']==0))]\n",
    "    sigFilesForPlotting.append(signalForPlotting)\n",
    "\n",
    "### UNCOMMENT FOR PRODUCING PRE-BDT plots ###    \n",
    "#plotVariables(sigFilesForPlotting, sigMasses, DYforPlotting, allTrainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff00387-f54d-4a4f-8cbe-c5d621d82183",
   "metadata": {},
   "source": [
    "THIS FUNCTION IS DEPRECATED. It was used to plot the BDT distributions of the training and testing datasets before we moved to k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffb705c2-fc56-48a3-80f5-1584397d32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  predAlt_labels = pd.DataFrame({'BDTscore':y_preds,'isSignal':labels,'eventWeights':weight})\n",
    "  predAltTrain_labels = pd.DataFrame({'BDTscore':y_predsTrain,'isSignal':labelsTrain,\n",
    "                                      'eventWeights':weightTrain})\n",
    "    \n",
    "  testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "  testSignalWeights_unity = testSignalWeights/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "  testBkgWeights_unity = testBkgWeights/(testBkgWeights.sum()*0.04)\n",
    "\n",
    "  trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "  trainSignalWeights_unity = trainSignalWeights/(trainSignalWeights.sum()*0.04)\n",
    "\n",
    "  trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "  trainBkgWeights_unity = trainBkgWeights/(trainBkgWeights.sum()*0.04)\n",
    "\n",
    "  axes = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity,\n",
    "                                                                                  label='Signal')# (train sample)')\n",
    "  predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity,\n",
    "                                                                                 label='Background')# (train sample)')  \n",
    "    \n",
    "\n",
    "  output = pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04))\n",
    "  print(output.value_counts())\n",
    "        \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  \n",
    "  npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "  counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity)\n",
    "  np_counts_bkg = np.asarray(counts_bkg)\n",
    "    \n",
    "  counts_bkg_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testBkgWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_bkg_sqrt.append(error)\n",
    "\n",
    "\n",
    "  npSignal_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,:].to_numpy()\n",
    "  counts_sig, binedges = np.histogram(npSignal_predAlt_labels[:,0], bins=bin_edges,weights=testSignalWeights_unity)\n",
    "  np_counts_sig = np.asarray(counts_sig)\n",
    "  counts_sig_sqrt = np.sqrt(np_counts_sig)\n",
    "\n",
    "  counts_sig_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npSignal_predAlt_labels[:,0], npSignal_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testSignalWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_sig_sqrt.append(error)\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"1/N dN/dx\", loc='top')\n",
    "  plt.ylim([0.0004,40])\n",
    "  plt.legend()\n",
    "  plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\".pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de5916-801e-4993-9397-a1b3c17535f2",
   "metadata": {},
   "source": [
    "This combines the datasets from the various k-folds before plotting the BDT distributions of the training vs testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "002b4292-5e26-4a88-860c-6fa882d9c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining_kFold(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  array_predAltTrain_labels = []  \n",
    "  trainSignalWeights_unity = []\n",
    "  trainBkgWeights_unity = []\n",
    "\n",
    "  predAlt_labels = pd.DataFrame({'BDTscore':y_preds,'isSignal':labels,'eventWeights':weight})\n",
    "  for i in range(0,len(y_predsTrain)):\n",
    "    predAltTrain_labels =  pd.DataFrame({'BDTscore':y_predsTrain[i],'isSignal':labelsTrain[i],\n",
    "                                      'eventWeights':weightTrain[i]})\n",
    "    \n",
    "    array_predAltTrain_labels.append(predAltTrain_labels)\n",
    "      \n",
    "    trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "    trainSignalWeights_unity.append(trainSignalWeights/(trainSignalWeights.sum()*0.04))\n",
    "\n",
    "    trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "    trainBkgWeights_unity.append(trainBkgWeights/(trainBkgWeights.sum()*0.04))\n",
    "    \n",
    "  testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "  testSignalWeights_unity = testSignalWeights/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "  testBkgWeights_unity = testBkgWeights/(testBkgWeights.sum()*0.04)\n",
    "\n",
    "  axes = array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity[0],\n",
    "                                                                                  label='Signal (train sample)')\n",
    "  array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity[0],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  array_predAltTrain_labels[1].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Green\",weights=trainBkgWeights_unity[1],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "\n",
    "  array_predAltTrain_labels[2].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Yellow\",weights=trainBkgWeights_unity[2],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "  array_predAltTrain_labels[3].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Orange\",weights=trainBkgWeights_unity[3],\n",
    "                                                                                 label='Bkg (train sample)')      \n",
    "  array_predAltTrain_labels[4].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Purple\",weights=trainBkgWeights_unity[4],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  output = pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04))\n",
    "  print(output.value_counts())\n",
    "        \n",
    "    \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  #axes.errorbars(bin_centers, )\n",
    "  \n",
    "  npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "  counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity)\n",
    "  np_counts_bkg = np.asarray(counts_bkg)\n",
    "  #counts_bkg_sqrt = np.sqrt(np_counts_bkg)/(testBkgWeights.sum()*0.04)\n",
    "    \n",
    "  counts_bkg_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testBkgWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_bkg_sqrt.append(error)\n",
    "\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_bkg, yerr=counts_bkg_sqrt, fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Background (test sample)')\n",
    "\n",
    "  npSignal_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,:].to_numpy()\n",
    "  counts_sig, binedges = np.histogram(npSignal_predAlt_labels[:,0], bins=bin_edges,weights=testSignalWeights_unity)\n",
    "  np_counts_sig = np.asarray(counts_sig)\n",
    "  counts_sig_sqrt = np.sqrt(np_counts_sig)\n",
    "#  counts_sig_sqrt = np.sqrt(np_counts_sig)/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  counts_sig_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npSignal_predAlt_labels[:,0], npSignal_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testSignalWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_sig_sqrt.append(error)\n",
    "    \n",
    "  plt.errorbar(bin_centers, counts_sig, yerr=counts_sig_sqrt, fmt='o', ecolor='Blue',\n",
    "               mfc='Blue',mec='Blue', label='Signal (test sample)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"1/N dN/dx\", loc='top')\n",
    "  plt.ylim([0.0004,10000])\n",
    "  #plt.legend(ncol=2)\n",
    "  plt.legend(loc='upper center',ncol=2)\n",
    "  plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\".pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1400d9-0ccb-4b6a-9f21-3add8bc0a280",
   "metadata": {},
   "source": [
    "This keeps the k-folds separate and plots the BDT distributions of the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "766e32d8-b349-4992-82d2-1a516629e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining_kFold_individaulTraining(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  array_predAltTrain_labels = []  \n",
    "  trainSignalWeights_unity = []\n",
    "  trainBkgWeights_unity = []\n",
    "  array_predAlt_labels = []  \n",
    "  testSignalWeights_unity = []\n",
    "  testBkgWeights_unity = []\n",
    "    \n",
    "  output = []\n",
    "  #npBkg_predAlt_labels = []\n",
    "  array_counts_bkg = []\n",
    "  array_counts_bkg_sqrt = []\n",
    "  array_counts_bkgTrain = []    \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.      \n",
    "\n",
    "  for i in range(0,len(y_predsTrain)):\n",
    "    predAltTrain_labels =  pd.DataFrame({'BDTscore':y_predsTrain[i],'isSignal':labelsTrain[i],\n",
    "                                      'eventWeights':weightTrain[i]})\n",
    "    predAlt_labels = pd.DataFrame({'BDTscore':y_preds[i],'isSignal':labels[i],'eventWeights':weight[i]})\n",
    "    \n",
    "    array_predAltTrain_labels.append(predAltTrain_labels)\n",
    "    array_predAlt_labels.append(predAlt_labels)\n",
    "      \n",
    "    trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "    trainSignalWeights_unity.append(trainSignalWeights)\n",
    "#    trainSignalWeights_unity.append(trainSignalWeights/(trainSignalWeights.sum()*0.04))\n",
    "\n",
    "    trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "    trainBkgWeights_unity.append(trainBkgWeights)\n",
    "#    trainBkgWeights_unity.append(trainBkgWeights/(trainBkgWeights.sum()*0.04))\n",
    "    \n",
    "    print(\"trainBkgWeights.sum()\")\n",
    "    print(trainBkgWeights.sum())\n",
    "    \n",
    "    testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "    testSignalWeights_unity.append(testSignalWeights)\n",
    "#    testSignalWeights_unity.append(testSignalWeights/(testSignalWeights.sum()*0.04))\n",
    "\n",
    "    testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "    testBkgWeights_unity.append(testBkgWeights)\n",
    "#    testBkgWeights_unity.append(testBkgWeights/(testBkgWeights.sum()*0.04))\n",
    "    \n",
    "    print(\"testBkgWeights.sum()\")\n",
    "    print(testBkgWeights.sum())\n",
    "    \n",
    "    output.append(pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04)))\n",
    "\n",
    "    npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "    \n",
    "    counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity[i])\n",
    "    array_counts_bkg.append(counts_bkg)\n",
    "    np_counts_bkg = np.asarray(counts_bkg)\n",
    "\n",
    "    npBkgTrain_predAlt_labels = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,:].to_numpy()\n",
    "    \n",
    "    counts_bkgTrain, binedges = np.histogram(npBkgTrain_predAlt_labels[:,0], bins=bin_edges,weights=trainBkgWeights_unity[i])\n",
    "    array_counts_bkgTrain.append(counts_bkgTrain)\n",
    "    \n",
    "    counts_bkg_sqrt = []\n",
    "    for bin_index in range(len(binedges) - 1):  \n",
    "         bin_left = binedges[bin_index]\n",
    "         bin_right = binedges[bin_index + 1]\n",
    "         in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "         # filter the weights to only those inside the bin\n",
    "         weights_in_bin = testBkgWeights_unity[i][in_bin]\n",
    "\n",
    "         # compute the error however you want\n",
    "         error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "         counts_bkg_sqrt.append(error)\n",
    "\n",
    "    array_counts_bkg_sqrt.append(counts_bkg_sqrt)\n",
    "\n",
    "  axes = array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity[0],\n",
    "                                                                                  label='Signal (train sample)')\n",
    "  array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity[0],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  array_predAltTrain_labels[1].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Green\",weights=trainBkgWeights_unity[1],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "\n",
    "  array_predAltTrain_labels[2].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Yellow\",weights=trainBkgWeights_unity[2],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "  array_predAltTrain_labels[3].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Orange\",weights=trainBkgWeights_unity[3],\n",
    "                                                                                 label='Bkg (train sample)')      \n",
    "  array_predAltTrain_labels[4].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Purple\",weights=trainBkgWeights_unity[4],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  print(\"Test Sample bin counts:\")  \n",
    "  print(array_counts_bkg[0][0])\n",
    "  print(array_counts_bkg[0][4])    \n",
    "\n",
    "  print(\"Train Sample bin counts:\")  \n",
    "  print(array_counts_bkgTrain[0][0])\n",
    "  print(array_counts_bkgTrain[0][4])    \n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[0], yerr=array_counts_bkg_sqrt[0], fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Background (test sample)')\n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[1], yerr=array_counts_bkg_sqrt[1], fmt='o', ecolor='Green',\n",
    "               mfc='Green',mec='Green',label='Background (test sample)')\n",
    "    \n",
    "  plt.errorbar(bin_centers, array_counts_bkg[2], yerr=array_counts_bkg_sqrt[2], fmt='o', ecolor='Yellow',\n",
    "               mfc='Yellow',mec='Yellow',label='Background (test sample)')\n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[3], yerr=array_counts_bkg_sqrt[3], fmt='o', ecolor='Orange',\n",
    "               mfc='Orange',mec='Orange',label='Background (test sample)')\n",
    "    \n",
    "  plt.errorbar(bin_centers, array_counts_bkg[4], yerr=array_counts_bkg_sqrt[4], fmt='o', ecolor='Purple',\n",
    "               mfc='Purple',mec='Purple',label='Background (test sample)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"# of events\", loc='top')\n",
    "  #plt.ylim([0.0004,10000])\n",
    "  #plt.legend(ncol=2)\n",
    "  #plt.legend(loc='upper center',ncol=2)\n",
    "  #plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\"_allTestSamples.pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6e466-2468-4d35-a097-6f813386cc4e",
   "metadata": {},
   "source": [
    "This is the part where we start the bdt training. Specifically, this performs the optimization of the BDT structural parameters.\n",
    "\n",
    "First, a filter is applied to the data to select events with a standalone deltaE over E less than -0.6, so that only probes with standalone muons that have at least 60% less energy are used in order to increase the relative fraction of background events. Also, events that are not on the cell edge are selected. The efficiency of these cuts is calculated for DY and signal (mceff and sigeff) so that the ROC curves can be later re-scaled to the correct fractions. \n",
    "\n",
    "Then the dataframe is split into a dataframe that contains the training values and is used in the optimizing of the BDT structure. The other dataframe contains the various systematic weights and is not used until the end when we need to apply the different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad43e9d2-8b0e-4a61-9871-efa6a69924a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%add_to Features\n",
    "@jit(target_backend='cuda')\n",
    "def Optimize(self,weightedInputs,sigMasses):\n",
    "    weightedInputs2 = weightedInputs.copy()\n",
    "    print(weightedInputs)\n",
    "    print(sigMasses)\n",
    "    \n",
    "    #for idx, dataset in enumerate(halfWeightedInputs):\n",
    "    for idx, dataset in enumerate(weightedInputs):\n",
    "        stacut = -0.6\n",
    "        print(len(dataset))\n",
    "        filter = (dataset['standaloneDEoverE']<stacut) & (dataset['cellEdgeDeta']>0.004) & (dataset['cellEdgeDphi']>0.016)\n",
    "        filteredData = dataset[filter]\n",
    "        print(len(filteredData))\n",
    "        bkg_filteredData = filteredData.copy()\n",
    "        bkg_filteredData = bkg_filteredData[bkg_filteredData['label']<0.5]\n",
    "        print(len(bkg_filteredData))\n",
    "        \n",
    "        filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "        filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_0', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_1', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_2', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_3', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_4', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_5', inplace=True, axis=1)\n",
    "        filteredData.drop('found_HEDepth_6', inplace=True, axis=1)    \n",
    "        print(\"Number of total filtered data\")\n",
    "        print(len(filteredData))\n",
    "        haveAll = filteredData.copy()\n",
    "        missingOneHit = filteredData.copy()\n",
    "        #  missingOneHit = missingOneHit[((missingOneHit['HEDepth_0']==0) | (missingOneHit['HEDepth_1']==0) | (missingOneHit['HEDepth_2']==0) | (missingOneHit['HEDepth_3']==0) | (missingOneHit['HEDepth_4']==0) | (missingOneHit['HEDepth_5']==0) | (missingOneHit['HEDepth_6']==0))]\n",
    "    for depth in range(7):\n",
    "        HEfilter = haveAll['HEDepth_'+str(depth)]!=0\n",
    "        haveAll = haveAll[HEfilter]\n",
    "        \n",
    "    print(\"Number with no missing hits\")\n",
    "    print(len(haveAll))\n",
    "    print(\"Number with one missing hit\")\n",
    "    print(len(missingOneHit))\n",
    "    print(\"Number of backgrond with one missing hit\")\n",
    "    bkg_missingOneHit = missingOneHit.copy()\n",
    "    bkg_missingOneHit = bkg_missingOneHit[bkg_missingOneHit['label']<0.5]\n",
    "    print(len(bkg_missingOneHit))\n",
    "    \n",
    "    missingOneHit_SystWeights = missingOneHit.copy()\n",
    "    missingOneHit_SystWeights.drop(['pt'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['eta'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['phi'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['staDR'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['staPhi'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['staE'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['staChi'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['cscDR'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_0'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_1'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_2'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_3'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_4'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_5'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['HEDepth_6'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['probeCharge'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['standaloneDEoverE'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['cscDRbyStation_0'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['cscDRbyStation_1'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['cscDRbyStation_2'],inplace=True, axis=1)\n",
    "    missingOneHit_SystWeights.drop(['cscDRbyStation_3'],inplace=True, axis=1)    \n",
    "    print(\"CHECKING WEIGHT VALUES\")\n",
    "    print(missingOneHit_SystWeights[missingOneHit_SystWeights['massPoint']=='0p4'])\n",
    "    \n",
    "    missingOneHit.drop(['PUupWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['PUdownWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['IDupWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['IDdownWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['ISOupWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['ISOdownWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['TrigUpWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['TrigDownWeight'],inplace=True, axis=1)    \n",
    "    missingOneHit.drop(['EnBinWeight'],inplace=True, axis=1) \n",
    "    missingOneHit.drop(['EventWeight_cscReweight'],inplace=True, axis=1) \n",
    "    print(missingOneHit)\n",
    "    \n",
    "    mc = dataset[dataset['label']<0.5]\n",
    "    mc_filter = mc.copy()\n",
    "    \n",
    "    \n",
    "    mceff = mc_filter[(mc_filter['standaloneDEoverE']<stacut) & (mc_filter['cellEdgeDeta']>0.004) & (mc_filter['cellEdgeDphi']>0.016)].shape[0]/mc.shape[0]# & \n",
    "                   #((mc_filter['HEDepth_0']==0) | (mc_filter['HEDepth_1']==0) | (mc_filter['HEDepth_2']==0) | (mc_filter['HEDepth_3']==0) | (mc_filter['HEDepth_4']==0) | (mc_filter['HEDepth_5']==0) | (mc_filter['HEDepth_6']==0))].shape[0]/mc.shape[0]\n",
    "    #                   ((mc_filter['HEDepth_0']==0) | (mc_filter['HEDepth_1']==0) | (mc_filter['HEDepth_2']==0) | (mc_filter['HEDepth_3']==0) | (mc_filter['HEDepth_4']==0) | (mc_filter['HEDepth_5']==0))].shape[0]/mc.shape[0]\n",
    "    signal = dataset[dataset['label']>0.5]\n",
    "    signal_filter = signal.copy()\n",
    "    \n",
    "    sigeff = signal_filter[(signal_filter['standaloneDEoverE']<stacut) & (signal_filter['cellEdgeDeta']>0.004) & (signal_filter['cellEdgeDphi']>0.016)].shape[0]/signal.shape[0]# &\n",
    "                   #((signal_filter['HEDepth_0']==0) | (signal_filter['HEDepth_1']==0) | (signal_filter['HEDepth_2']==0) | (signal_filter['HEDepth_3']==0) | (signal_filter['HEDepth_4']==0) | (signal_filter['HEDepth_5']==0) | (signal_filter['HEDepth_6']==0))].shape[0]/signal.shape[0]  \n",
    "    #                   ((signal_filter['HEDepth_0']==0) | (signal_filter['HEDepth_1']==0) | (signal_filter['HEDepth_2']==0) | (signal_filter['HEDepth_3']==0) | (signal_filter['HEDepth_4']==0) | (signal_filter['HEDepth_5']==0))].shape[0]/signal.shape[0]  \n",
    "    \n",
    "    self.best_estimator, self.best_params, self.train, self.test, self.trainWeight, self.testWeight, self.raw_pos_weight, self.train_array, self.test_array, self.test_masses = optimize_hyper_xg(missingOneHit,self.TrainingFeatures,1)\n",
    "    self.missingOneHit=missingOneHit\n",
    "    self.missingOneHit_SystWeights=missingOneHit_SystWeights\n",
    "    self.haveAll=haveAll\n",
    "    self.mceff=mceff\n",
    "    self.sigeff=sigeff\n",
    "    \n",
    "    #  best_estimator, best_params, train, test, trainWeight, testWeight, raw_pos_weight, train_array, test_array, test_masses = optimize_hyper_xg(filteredData,allTrainingFeatures,1)\n",
    "    print(\"FINISHED optimize_hyper_xg\")\n",
    "    print(\"raw_pos_weight\")\n",
    "    print(self.raw_pos_weight)\n",
    "    \n",
    "nFolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059301e1-d583-4bd3-a96a-37a1abe66690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[             pt       eta       phi     staDR    staPhi        staE    staChi  \\\n",
      "0     36.234983  1.680514  0.178885  0.054614  0.048367   46.247898  7.220243   \n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517   59.415211  0.753268   \n",
      "2     50.687919  2.220909  0.804715  0.068122 -0.068010  115.821770  1.541431   \n",
      "3     49.187752  1.931597 -1.972739  0.021346 -0.021044  259.733459  1.857932   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750   44.757290  3.293752   \n",
      "...         ...       ...       ...       ...       ...         ...       ...   \n",
      "2646  25.956697  1.679126  2.973396  0.044959  0.040870   49.544533  1.280769   \n",
      "2647  46.760750 -1.726106  1.106414  0.096718  0.096210   43.609550  1.489939   \n",
      "2648  47.668459 -1.927518 -0.253790  0.010157  0.010152  136.373871  0.565698   \n",
      "2649  43.922674  2.088944  2.485391  0.627433  0.627275    7.145236  0.686833   \n",
      "2650  30.699898 -2.082791  2.693759  0.016342 -0.016335  183.138077  0.521087   \n",
      "\n",
      "         cscDR  disStaChi2  disStadPhi  ...  IDdownWeight  ISOupWeight  \\\n",
      "0     0.003537    3.989355    0.032591  ...      1.563022     1.610880   \n",
      "1     0.001801    0.740526    0.057978  ...      1.858269     1.915039   \n",
      "2     0.003291    1.426785   -0.318916  ...      1.132262     1.166831   \n",
      "3     0.001489    1.855981   -0.020965  ...      1.126282     1.160835   \n",
      "4     0.005710    3.353067    0.060907  ...      1.031290     1.062992   \n",
      "...        ...         ...         ...  ...           ...          ...   \n",
      "2646  0.009030    1.247381   -0.133403  ...      1.002299     1.033748   \n",
      "2647  0.003230    1.371207   -0.129013  ...      1.006906     1.037653   \n",
      "2648  0.001793    0.566054   -0.010258  ...      0.963541     0.993162   \n",
      "2649  0.024621    0.704607   -0.666378  ...      0.978059     1.007925   \n",
      "2650  0.002016    2.014431    0.433218  ...      0.985466     1.015579   \n",
      "\n",
      "      ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  label  \\\n",
      "0          1.547568      1.594822        1.563308     1.579065      0   \n",
      "1          1.839821      1.895975        1.858508     1.877242      0   \n",
      "2          1.121043      1.155236        1.132408     1.143822      0   \n",
      "3          1.115428      1.149526        1.126511     1.138018      0   \n",
      "4          1.020975      1.052623        1.031133     1.041878      0   \n",
      "...             ...           ...             ...          ...    ...   \n",
      "2646       0.991691      1.022785        1.002437     1.042172      1   \n",
      "2647       0.996915      1.027304        1.007061     1.082726      1   \n",
      "2648       0.954256      0.983307        0.963917     0.979962      1   \n",
      "2649       0.968354      0.997872        0.978209     0.994485      1   \n",
      "2650       0.975635      1.005668        0.985345     1.044287      1   \n",
      "\n",
      "      trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0           1.579065                 1.259129        NaN  \n",
      "1           1.877242                 1.745842        NaN  \n",
      "2           1.143822                 1.063759        NaN  \n",
      "3           1.138018                 1.058362        NaN  \n",
      "4           1.041878                 0.865819        NaN  \n",
      "...              ...                      ...        ...  \n",
      "2646        7.197788                 1.012611        1p0  \n",
      "2647        7.230280                 1.017182        1p0  \n",
      "2648        6.920577                 0.973612        1p0  \n",
      "2649        7.023137                 0.988040        1p0  \n",
      "2650        7.076209                 0.995507        1p0  \n",
      "\n",
      "[360943 rows x 53 columns]]\n",
      "['0p2', '0p4', '0p6', '0p8', '1p0']\n",
      "360943\n",
      "121178\n",
      "111858\n",
      "Number of total filtered data\n",
      "121178\n",
      "Number with no missing hits\n",
      "101163\n",
      "Number with one missing hit\n",
      "121178\n",
      "Number of backgrond with one missing hit\n",
      "111858\n",
      "CHECKING WEIGHT VALUES\n",
      "      disStaChi2  disStadPhi  disStadEta  disStaDEoverE    refitStaE  \\\n",
      "1       0.599764    0.284023    0.046362      -0.920168    18.488047   \n",
      "6       1.053658    0.523033    0.005534      -0.970004     6.782049   \n",
      "7       0.440225    0.233818    0.009313      -0.849821    32.642532   \n",
      "10     -1.000000  -10.000000  -10.000000      10.000000    -1.000000   \n",
      "15      0.559460   -0.468511   -0.073674      -0.915723  1104.896851   \n",
      "...          ...         ...         ...            ...          ...   \n",
      "8813    0.504356   -0.121883   -0.011198      -0.722075    26.885509   \n",
      "8816    0.588569   -0.240411   -0.078948      -0.920085    10.592689   \n",
      "8827    1.642195   -0.441849    0.435853      -0.938961     6.221093   \n",
      "8832    2.405797    0.094489    0.187795      -0.879096    54.011978   \n",
      "8836    0.537106   -0.095522    0.006290      -0.740165    60.930439   \n",
      "\n",
      "      refitStaChi2  refitStadPhi  refitStadEta  refitStaDEoverE  EventWeight  \\\n",
      "1         0.656504      0.244145      0.046667        -0.920168     1.002763   \n",
      "6         1.221905      0.451217      0.001146        -0.970004     0.984855   \n",
      "7         2.120833     -0.264739      0.032560        -0.849821     0.967017   \n",
      "10       -1.000000    -10.000000    -10.000000        10.000000     0.945212   \n",
      "15        3.433933      0.188076     -0.128803        -0.915723     1.031637   \n",
      "...            ...           ...           ...              ...          ...   \n",
      "8813      0.469084     -0.136371     -0.011568        -0.722075     0.946963   \n",
      "8816      0.658033     -0.243636     -0.090645        -0.920085     1.011301   \n",
      "8827      1.084464     -0.706002     -0.007869        -0.938961     0.945737   \n",
      "8832      3.165587      0.025751     -0.010334        -0.879096     1.023582   \n",
      "8836      0.641197     -0.070803      0.006676        -0.740165     0.991448   \n",
      "\n",
      "      ...  IDdownWeight  ISOupWeight  ISOdownWeight  TrigUpWeight  \\\n",
      "1     ...      0.992628     1.022907       0.982819      1.012769   \n",
      "6     ...      0.974906     1.004630       0.965276      0.994655   \n",
      "7     ...      0.957248     0.986491       0.947736      0.976639   \n",
      "10    ...      0.935685     0.964287       0.926327      0.954875   \n",
      "15    ...      1.021215     1.052399       1.011081      1.041902   \n",
      "...   ...           ...          ...            ...           ...   \n",
      "8813  ...      0.937413     0.966057       0.928061      0.956630   \n",
      "8816  ...      1.001026     1.031667       0.991137      1.021392   \n",
      "8827  ...      0.936205     0.964903       0.926763      0.955405   \n",
      "8832  ...      1.013026     1.044199       1.003171      1.033933   \n",
      "8836  ...      0.981193     1.011310       0.971782      1.001321   \n",
      "\n",
      "      TrigDownWeight  EnBinWeight  label  trainingWeight  \\\n",
      "1           0.992756     1.025530      1        2.137365   \n",
      "6           0.975055     0.971157      1        2.099194   \n",
      "7           0.957394     1.010136      1        2.061173   \n",
      "10          0.935549     0.918078      1        2.014697   \n",
      "15          1.021372     1.037352      1        2.198909   \n",
      "...              ...          ...    ...             ...   \n",
      "8813        0.937297     0.942951      1        2.018430   \n",
      "8816        1.001209     1.016903      1        2.155563   \n",
      "8827        0.936069     0.932300      1        2.015816   \n",
      "8832        1.013232     1.016325      1        2.181741   \n",
      "8836        0.981575     1.000093      1        2.113247   \n",
      "\n",
      "      EventWeight_cscReweight  massPoint  \n",
      "1                    1.002763        0p4  \n",
      "6                    0.984855        0p4  \n",
      "7                    0.967017        0p4  \n",
      "10                   0.945212        0p4  \n",
      "15                   1.031637        0p4  \n",
      "...                       ...        ...  \n",
      "8813                 0.946963        0p4  \n",
      "8816                 1.011301        0p4  \n",
      "8827                 0.945737        0p4  \n",
      "8832                 1.023582        0p4  \n",
      "8836                 0.991448        0p4  \n",
      "\n",
      "[2268 rows x 23 columns]\n",
      "             pt       eta       phi     staDR    staPhi       staE     staChi  \\\n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517  59.415211   0.753268   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750  44.757290   3.293752   \n",
      "7     50.515547  1.671956 -0.568947  0.283308 -0.002418  25.153557  60.916972   \n",
      "8     44.155022 -2.124426  0.979609  0.405137  0.403815  11.643636   5.180820   \n",
      "9     80.522550 -2.381075  2.027694  0.114773 -0.114573  67.719337   4.629184   \n",
      "...         ...       ...       ...       ...       ...        ...        ...   \n",
      "2627  37.887690 -2.285758 -1.949500  0.229738  0.228045  22.527164   1.054400   \n",
      "2633  75.051499  1.966172  0.888832  0.094592  0.094482  47.067753   0.440357   \n",
      "2634  45.202535 -2.280795 -1.970599  0.374488  0.350142  12.592485   1.356520   \n",
      "2635  30.082905  1.989121  1.851301  0.291456  0.281572  15.634134   1.483363   \n",
      "2649  43.922674  2.088944  2.485391  0.627433  0.627275   7.145236   0.686833   \n",
      "\n",
      "         cscDR  disStaChi2  disStadPhi  ...  probeCharge  standaloneDEoverE  \\\n",
      "1     0.001801    0.740526    0.057978  ...          1.0          -0.660127   \n",
      "4     0.005710    3.353067    0.060907  ...          1.0          -0.775219   \n",
      "7     0.006505    0.343895    0.271699  ...         -1.0          -0.819275   \n",
      "8     0.004045    4.127022    0.529807  ...          1.0          -0.937862   \n",
      "9     0.002713    2.541522    0.626740  ...         -1.0          -0.845815   \n",
      "...        ...         ...         ...  ...          ...                ...   \n",
      "2627  0.013259    1.005888    0.256361  ...          1.0          -0.880304   \n",
      "2633  0.004402    0.436166   -0.096381  ...         -1.0          -0.827786   \n",
      "2634  0.027784    1.742073   -0.224069  ...         -1.0          -0.943645   \n",
      "2635  0.008114    1.253417    0.318799  ...          1.0          -0.860406   \n",
      "2649  0.024621    0.704607   -0.666378  ...         -1.0          -0.960324   \n",
      "\n",
      "      cscDRbyStation_0  cscDRbyStation_1  cscDRbyStation_2  cscDRbyStation_3  \\\n",
      "1             0.010030          0.001801          0.009905          0.013251   \n",
      "4             0.021389          0.005710          0.008495          0.011281   \n",
      "7             0.012604          0.006505          0.011184          0.015411   \n",
      "8             4.416383          0.004045          0.005047          0.008676   \n",
      "9             0.006248          0.007338          0.002713          0.003895   \n",
      "...                ...               ...               ...               ...   \n",
      "2627          0.013259          0.019142          0.021324          0.026026   \n",
      "2633          0.004941          0.004402          0.006968          0.009218   \n",
      "2634          0.027784          0.069402          0.077509          0.088933   \n",
      "2635          0.008114          0.032895          0.032780          0.039157   \n",
      "2649          0.027383          0.073306          0.031235          0.024621   \n",
      "\n",
      "      EventWeight  label  trainingWeight  massPoint  \n",
      "1        1.877242      0        1.877242        NaN  \n",
      "4        1.041878      0        1.041878        NaN  \n",
      "7        3.931283      0        3.931283        NaN  \n",
      "8        0.558645      0        0.558645        NaN  \n",
      "9        1.034571      0        1.034571        NaN  \n",
      "...           ...    ...             ...        ...  \n",
      "2627     0.986820      1        7.014460        1p0  \n",
      "2633     1.021539      1        7.261249        1p0  \n",
      "2634     0.975704      1        6.935445        1p0  \n",
      "2635     0.947398      1        6.734245        1p0  \n",
      "2649     0.988040      1        7.023137        1p0  \n",
      "\n",
      "[121178 rows x 34 columns]\n",
      "['pt', 'eta', 'phi', 'staDR', 'staPhi', 'staE', 'staChi', 'cscDR', 'HEDepth_0', 'HEDepth_1', 'HEDepth_2', 'HEDepth_3', 'HEDepth_4', 'HEDepth_5', 'HEDepth_6', 'probeCharge', 'standaloneDEoverE', 'cscDRbyStation_0', 'cscDRbyStation_1', 'cscDRbyStation_2', 'cscDRbyStation_3', 'trainingWeight']\n",
      "['pt', 'eta', 'phi', 'staDR', 'staPhi', 'staE', 'staChi', 'cscDR', 'HEDepth_0', 'HEDepth_1', 'HEDepth_2', 'HEDepth_3', 'HEDepth_4', 'HEDepth_5', 'HEDepth_6', 'probeCharge', 'standaloneDEoverE', 'cscDRbyStation_0', 'cscDRbyStation_1', 'cscDRbyStation_2', 'cscDRbyStation_3']\n",
      "21\n",
      "dict_keys(['explained_variance', 'r2', 'max_error', 'matthews_corrcoef', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'positive_likelihood_ratio', 'neg_negative_likelihood_ratio', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])\n",
      "96942\n",
      "21\n",
      "RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBRegressor(base_score=None, booster='gbtree',\n",
      "                                          callbacks=None,\n",
      "                                          colsample_bylevel=None,\n",
      "                                          colsample_bynode=None,\n",
      "                                          colsample_bytree=None,\n",
      "                                          early_stopping_rounds=None,\n",
      "                                          enable_categorical=False,\n",
      "                                          eval_metric='logloss',\n",
      "                                          feature_types=None, gamma=None,\n",
      "                                          gpu_id=None, grow_policy=None,\n",
      "                                          importance_type=None,\n",
      "                                          interaction_constraints=None,\n",
      "                                          learn...\n",
      "                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001E5B95C4990>,\n",
      "                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001E5BA339990>,\n",
      "                                        'subsample': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001E59523EA50>},\n",
      "                   scoring=make_scorer(neg_mean_squared_weighted_error, weights=[0.0095051  0.56866266 0.5036373  ... 1.03652336 1.05171957 1.43130329]),\n",
      "                   verbose=1)\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    all.Optimize(weightedInputs,sigMasses)\n",
    "    new.Optimize(weightedInputs,sigMasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5fb38-2517-457b-b83d-b8c65f7ab2ac",
   "metadata": {},
   "source": [
    "This performs the final BDT training through the k-fold validation. The various training datasets, testing datasets, and BDTs from the different folds are kept track of. Along with the event weights and labels of the various datasets. This information is then used to create arrays of the BDT predictions for all signal and DY events.\n",
    "\n",
    "And finally, the BDT distributions of the training and testing datasets are plotted along with the importance of the various training variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff610fe9-1fce-4122-a5ae-fc6aa02571b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%add_to Features\n",
    "@jit(target_backend='cuda')\n",
    "def finalTraining(self,nFolds):\n",
    "    curves = []\n",
    "    importance = []\n",
    "    sigPreds = []\n",
    "    newGenPreds = [] \n",
    "\n",
    "    mceff=self.mceff\n",
    "    sigeff=self.sigeff\n",
    "    \n",
    "    bdtArray, array_train, array_test, array_train_array, array_test_array, array_train_lbl, array_test_lbl, array_trainWeight, array_testWeight, array_testMasses, array_SystWeights = kFoldCrossValidation(self.best_params,self.missingOneHit,self.TrainingFeatures,nFolds,self.missingOneHit_SystWeights)\n",
    "    self.bdtArray, self.array_train, self.array_test, self.array_train_array, self.array_test_array, self.array_train_lbl, self.array_test_lbl, self.array_trainWeight, self.array_testWeight, self.array_testMasses, self.array_SystWeights = bdtArray, array_train, array_test, array_train_array, array_test_array, array_train_lbl, array_test_lbl, array_trainWeight, array_testWeight, array_testMasses, array_SystWeights\n",
    "    \n",
    "    #  bdtArray, array_train, array_test, array_train_array, array_test_array, array_train_lbl, array_test_lbl, array_trainWeight, array_testWeight, array_testMasses = kFoldCrossValidation(best_params,filteredData,allTrainingFeatures,nFolds)\n",
    "    print(len(bdtArray[0].evals_result()))\n",
    "    print(bdtArray[0].evals_result())\n",
    "    print(\"len(array_test_array)\")\n",
    "    print(len(array_test_array))\n",
    "    \n",
    "    sumLogLoss = 0\n",
    "    for i in range(0,nFolds):\n",
    "        eval_results = bdtArray[i].evals_result()\n",
    "        sumLogLoss = sumLogLoss + eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1]\n",
    "    meanLogLoss = sumLogLoss/nFolds\n",
    "    print(\"meanLogLoss\")\n",
    "    print(meanLogLoss)\n",
    "    \n",
    "    sumSquaredLogLoss = 0\n",
    "    for i in range(0,nFolds): \n",
    "        eval_results = bdtArray[i].evals_result()\n",
    "        sumSquaredLogLoss = sumSquaredLogLoss + (meanLogLoss - eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1])**2\n",
    "    \n",
    "    stdLogLoss = mth.sqrt(sumSquaredLogLoss/nFolds)\n",
    "    print(\"stdLogLoss\")\n",
    "    print(stdLogLoss)\n",
    "    \n",
    "    #print(eval_results['validation_0']['logloss'])\n",
    "    #print(eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1])\n",
    "    #for key, value in eval_results['validation_0'].items():\n",
    "    #      print(key, value)\n",
    "    \n",
    "    haveAll_x,haveAll_y,haveAll_masses = format_data(self.haveAll,self.TrainingFeatures)\n",
    "    haveAll_weights = haveAll_x[:,haveAll_x.shape[1]-1]\n",
    "    haveAll_x = np.delete(haveAll_x,haveAll_x.shape[1]-1,1)            \n",
    "    \n",
    "    haveAll_pred = []\n",
    "    for i in range(0,nFolds):\n",
    "        haveAll_pred.append(bdtArray[i].predict(haveAll_x))\n",
    "    #    print(\"BDT i\")\n",
    "    #    print (i)\n",
    "    #    for j in range(0,len(haveAll_pred[i])):\n",
    "    #        print(j)\n",
    "    #        print(haveAll_pred[i][j])\n",
    "    \n",
    "    \n",
    "    print(\"Number of haveAll events\")\n",
    "    print(len(haveAll_pred[0]))\n",
    "    haveAll_y = haveAll_y.flatten()\n",
    "    print(haveAll_y)\n",
    "    print(haveAll_weights)\n",
    "    nBkg_haveAll = 0\n",
    "    for i in range(0,len(haveAll_pred[0])):\n",
    "        if haveAll_y[i] == 0: nBkg_haveAll = nBkg_haveAll + 1\n",
    "    \n",
    "    print(nBkg_haveAll)\n",
    "    \n",
    "    y_preds, labels, weights, masses_labels, testEvents_array, BDToutput_systWeights = combiningKFolds(bdtArray,array_test,array_test_array,array_testWeight,array_testMasses,array_train,array_train_array,array_trainWeight,nFolds, array_SystWeights)\n",
    "    self.y_preds, self.labels, self.weights, self.masses_labels, self.testEvents_array, self.BDToutput_systWeights = y_preds, labels, weights, masses_labels, testEvents_array, BDToutput_systWeights\n",
    "    print(len(BDToutput_systWeights))\n",
    "    print(\"DEBUG\")\n",
    "    print(len(testEvents_array))\n",
    "    print(len(y_preds))\n",
    "    print(len(weights))\n",
    "    print(len(labels))\n",
    "    nBkg_AllEvents = 0\n",
    "    for i in range(0,len(y_preds)):\n",
    "        if labels[i] == 0: nBkg_AllEvents = nBkg_AllEvents + 1\n",
    "    \n",
    "    print(\"Number of Bkg events total\")\n",
    "    print(nBkg_AllEvents)\n",
    "    \n",
    "    fpralt, tpralt, thresholdsalt = roc_curve(labels, y_preds,sample_weight=weights) \n",
    "    \n",
    "    print(\"DEBUGGING\")\n",
    "    print(fpralt)\n",
    "    print(len(fpralt))\n",
    "    \n",
    "    \n",
    "    ##### CHARGE NOT USED IN BDT ######\n",
    "    #  filteredData.drop('probeCharge', inplace=True, axis=1)\n",
    "    #  best_estimator_noC, best_params_noC, train_noC, test_noC, trainWeight_noC, testWeight_noC, raw_pos_weight_noC, train_array_noC, test_array_noC, test_masses_noC = optimize_hyper_xg(filteredData,allTrainingFeatures_noC,1)\n",
    "    \n",
    "    #  y_predsAlt_noC = best_estimator_noC.predict(test_array_noC)\n",
    "    #  labelsalt_noC = test_noC.get_label()\n",
    "    #  fpralt_noC, tpralt_noC, thresholdsalt_noC = roc_curve(labelsalt_noC, y_predsAlt_noC,sample_weight=testWeight_noC) \n",
    "    \n",
    "    #  y_predsAltTrain_noC = best_estimator_noC.predict(train_array_noC)\n",
    "    #  labelsaltTrain_noC = train_noC.get_label()    \n",
    "    #####################################\n",
    "         \n",
    "    \n",
    "    #Using predictions of testing dataset\n",
    "    bkg_predsAlt = []\n",
    "    sig0p2_predsAlt = []\n",
    "    sig0p4_predsAlt = []\n",
    "    sig0p6_predsAlt = []\n",
    "    sig0p8_predsAlt = []\n",
    "    sig1p0_predsAlt = []\n",
    "    #  sig2p0_predsAlt = []   \n",
    "    for idx2, BDTscore in enumerate(y_preds):\n",
    "        if labels[idx2] == 0: bkg_predsAlt.append(BDTscore)\n",
    "        else:\n",
    "            if masses_labels[idx2] == \"0p2\": sig0p2_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p4\": sig0p4_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p6\": sig0p6_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p8\": sig0p8_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"1p0\": sig1p0_predsAlt.append(BDTscore)\n",
    "    #            elif masses_labels[idx2] == \"2p0\": sig2p0_predsAlt.append(BDTscore)\n",
    "    \n",
    "    ###UNCOMMENT TO PLOT SIGNAL LIKE BACKGROUND EVENTS    \n",
    "    #plotVariables(sig_SignalLikeEvents, sigMasses, df_bkg_SignalLikeEvents, allTrainingFeatures)    \n",
    "    \n",
    "    #plotOverTraining(y_predsAlt,labelsalt,array_testWeight[0],y_predsAltTrain,labelsaltTrain,array_trainWeight[0],\"allVariables\")\n",
    "    #plotOverTraining(y_predsAlt_noC,labelsalt_noC,testWeight_noC,y_predsAltTrain_noC,labelsaltTrain_noC,trainWeight_noC,\"noTrackCharge\")\n",
    "    \n",
    "    \n",
    "    newGenPreds.append(bkg_predsAlt)\n",
    "    ###When using the fixBiasing samples###\n",
    "    sigPreds.append(sig0p4_predsAlt)\n",
    "    sigPreds.append(sig0p8_predsAlt)\n",
    "    sigPreds.append(sig0p6_predsAlt)\n",
    "    sigPreds.append(sig0p2_predsAlt)\n",
    "    sigPreds.append(sig1p0_predsAlt)\n",
    "    ###When using the ecalBrem samples###\n",
    "    #sigPreds.append(sig0p4_predsAlt)\n",
    "    #sigPreds.append(sig0p6_predsAlt)\n",
    "    #sigPreds.append(sig0p2_predsAlt)\n",
    "    #sigPreds.append(sig0p8_predsAlt)\n",
    "    #sigPreds.append(sig1p0_predsAlt)    \n",
    "    \n",
    "    #  importance.append(xgb.plot_importance(bst))\n",
    "    importance.append(xgb.plot_importance(bdtArray[0]))\n",
    "    \n",
    "    Text_yticklabels = list(importance[0].get_yticklabels())\n",
    "    dict_features = dict(enumerate(self.TrainingFeatures[0:len(self.TrainingFeatures)-1]))\n",
    "    lst_yticklabels = [ Text_yticklabels[i].get_text().lstrip('f') for i in range(len(Text_yticklabels))]\n",
    "    lst_yticklabels = [ dict_features[int(i)] for i in lst_yticklabels]\n",
    "    \n",
    "    importance[0].set_yticklabels(lst_yticklabels)\n",
    "    print(dict_features)\n",
    "    plt.show()\n",
    "    \n",
    "    #importance.append(xgb.plot_importance(best_estimator_noC))\n",
    "    \n",
    "    #Text_yticklabels = list(importance[1].get_yticklabels())\n",
    "    #dict_features = dict(enumerate(allTrainingFeatures_noC[0:len(allTrainingFeatures_noC)-1]))\n",
    "    #lst_yticklabels = [ Text_yticklabels[i].get_text().lstrip('f') for i in range(len(Text_yticklabels))]\n",
    "    #lst_yticklabels = [ dict_features[int(i)] for i in lst_yticklabels]\n",
    "    \n",
    "    #importance[1].set_yticklabels(lst_yticklabels)\n",
    "    #print(dict_features)\n",
    "    #plt.show()\n",
    "    \n",
    "    print(\"DEBUGGING\")\n",
    "    print(len(weights))\n",
    "    \n",
    "    curves.append([fpralt,tpralt,fpralt,tpralt,mceff,sigeff,fpralt,tpralt,mceff,sigeff])\n",
    "    self.curves=curves\n",
    "    self.masses_labels=masses_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019de866-65cb-4b81-9c42-f22de479f3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    all.finalTraining(nFolds)\n",
    "    new.finalTraining(nFolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962cc07-078f-4175-a749-6d8633cae383",
   "metadata": {},
   "source": [
    "This plots the training variables for the high BDT events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72887b9f-cdfc-4f2a-a66a-4c5ca9141233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  #Plot the signal like events. These are selected in the above cell\n",
    "  bkg_SignalLikeEvents = []\n",
    "  sig0p2_SignalLikeEvents = []     \n",
    "  sig0p4_SignalLikeEvents = []     \n",
    "  sig0p6_SignalLikeEvents = []     \n",
    "  sig0p8_SignalLikeEvents = []     \n",
    "  sig1p0_SignalLikeEvents = []     \n",
    "#  sig2p0_SignalLikeEvents = []         \n",
    "  print(\"LOOKING FOR SIGNAL LIKE EVENTS\")    \n",
    "  print(len(all.masses_labels))\n",
    "  print(len(all.y_preds))\n",
    "  print(len(all.testEvents_array))\n",
    "  print(len(all.weights))\n",
    "\n",
    "  ################################################################################################################# I think the below is right for comparisons\n",
    "  testEvents_array = new.testEvents_array\n",
    "  weights = new.weights\n",
    "  y_preds = all.y_preds\n",
    "  labels = all.labels \n",
    "  masses_labels = all.masses_labels\n",
    " \n",
    "\n",
    "  for idx2, BDTscore in enumerate(y_preds):\n",
    "      if BDTscore > 0.95 and labels[idx2]==0:\n",
    "            print(idx2)\n",
    "            print(BDTscore)\n",
    "            bkg_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "      elif BDTscore > 0.95 and labels[idx2]==1:\n",
    "            if masses_labels[idx2] == \"0p2\": sig0p2_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p4\": sig0p4_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p6\": sig0p6_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p8\": sig0p8_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"1p0\": sig1p0_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "#            elif masses_labels[idx2] == \"2p0\": sig2p0_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    " \n",
    "  print(\"Number of background events\")\n",
    "  print(len(bkg_SignalLikeEvents))\n",
    "  print(len(sig0p8_SignalLikeEvents))\n",
    "  np_bkg_SignalLikeEvents = np.array(bkg_SignalLikeEvents)\n",
    "  np_sig0p2_SignalLikeEvents = np.array(sig0p2_SignalLikeEvents)\n",
    "  np_sig0p4_SignalLikeEvents = np.array(sig0p4_SignalLikeEvents)\n",
    "  np_sig0p6_SignalLikeEvents = np.array(sig0p6_SignalLikeEvents)\n",
    "  np_sig0p8_SignalLikeEvents = np.array(sig0p8_SignalLikeEvents)\n",
    "  np_sig1p0_SignalLikeEvents = np.array(sig1p0_SignalLikeEvents)\n",
    "#  np_sig2p0_SignalLikeEvents = np.array(sig2p0_SignalLikeEvents)\n",
    "  print(len(newTrainingFeatures))\n",
    "  print(np_sig0p8_SignalLikeEvents)\n",
    "  df_bkg_SignalLikeEvents = pd.DataFrame(np_bkg_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "  df_sig0p2_SignalLikeEvents = pd.DataFrame(np_sig0p2_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "  df_sig0p4_SignalLikeEvents = pd.DataFrame(np_sig0p4_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "  df_sig0p6_SignalLikeEvents = pd.DataFrame(np_sig0p6_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "  df_sig0p8_SignalLikeEvents = pd.DataFrame(np_sig0p8_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "  df_sig1p0_SignalLikeEvents = pd.DataFrame(np_sig1p0_SignalLikeEvents, columns=newTrainingFeatures)\n",
    "#  df_sig2p0_SignalLikeEvents = pd.DataFrame(np_sig2p0_SignalLikeEvents, columns=allTrainingFeatures)    \n",
    "    \n",
    "  sig_SignalLikeEvents = []  \n",
    "  for mass in sigMasses:\n",
    "    if mass == \"0p2\": sig_SignalLikeEvents.append(df_sig0p2_SignalLikeEvents)\n",
    "    if mass == \"0p4\": sig_SignalLikeEvents.append(df_sig0p4_SignalLikeEvents)\n",
    "    if mass == \"0p6\": sig_SignalLikeEvents.append(df_sig0p6_SignalLikeEvents)\n",
    "    if mass == \"0p8\": sig_SignalLikeEvents.append(df_sig0p8_SignalLikeEvents)\n",
    "    if mass == \"1p0\": sig_SignalLikeEvents.append(df_sig1p0_SignalLikeEvents)\n",
    "#    if mass == \"2p0\": sig_SignalLikeEvents.append(df_sig2p0_SignalLikeEvents)\n",
    "    \n",
    "    \n",
    "  plotVariables(sig_SignalLikeEvents, sigMasses, df_bkg_SignalLikeEvents, newTrainingFeatures)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e9a7d-c6cc-4da5-a707-01dc7083d4e2",
   "metadata": {},
   "source": [
    "BDT results for the same sign MC events. Not used anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4608-26d4-4c28-9e67-2f84e6c18f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ''' stacut = -0.6\n",
    "  filter = (weightedMcInputs_SameSign['standaloneDEoverE']<stacut) & (weightedMcInputs_SameSign['cellEdgeDeta']>0.004) & (weightedMcInputs_SameSign['cellEdgeDphi']>0.016)\n",
    "  filteredData = weightedMcInputs_SameSign[filter]\n",
    "\n",
    "  print(len(filteredData))\n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_SameSign = filteredData.copy()\n",
    " # missingOneHit_SameSign = missingOneHit_SameSign[((missingOneHit_SameSign['HEDepth_0']==0) | (missingOneHit_SameSign['HEDepth_1']==0) | (missingOneHit_SameSign['HEDepth_2']==0) | (missingOneHit_SameSign['HEDepth_3']==0) | (missingOneHit_SameSign['HEDepth_4']==0) | (missingOneHit_SameSign['HEDepth_5']==0) | (missingOneHit_SameSign['HEDepth_6']==0))]\n",
    "\n",
    "  print(len(missingOneHit_SameSign))\n",
    "  missingOneHit_SameSign_x,missingOneHit_SameSign_y,missingOneHit_SameSign_massess = format_data(missingOneHit_SameSign,allTrainingFeatures)\n",
    "  missingOneHit_SameSign_weights = missingOneHit_SameSign_x[:,missingOneHit_SameSign_x.shape[1]-1]\n",
    "  missingOneHit_SameSign_x = np.delete(missingOneHit_SameSign_x,missingOneHit_SameSign_x.shape[1]-1,1)            \n",
    "\n",
    "  SameSign_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    SameSign_pred.append(bdtArray[i].predict(missingOneHit_SameSign_x))\n",
    "  \n",
    "  print(SameSign_pred)\n",
    "  np.histogram(SameSign_pred[1],bins=np.arange(0,1.01,0.04))\n",
    "#  axes = SameSign_pred[0].hist(column='BDTscore',log=True,\n",
    "#                               bins=np.arange(0,1.01,0.04),\n",
    "#                               label='Same Sign BDT 0')   '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433649f2-210f-4d36-8431-5f296f20aade",
   "metadata": {},
   "source": [
    "BDT results for the same sign data events. Used to predict the muPX background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8154920-1215-4c55-b36b-813be1761970",
   "metadata": {},
   "outputs": [],
   "source": [
    "  '''stacut = -0.6\n",
    "  filter = (dataInputs_SameSign['standaloneDEoverE']<stacut) & (dataInputs_SameSign['cellEdgeDeta']>0.004) & (dataInputs_SameSign['cellEdgeDphi']>0.016)\n",
    "  filteredData = dataInputs_SameSign[filter]\n",
    "    \n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_data_SameSign = filteredData.copy()\n",
    "\n",
    "  data_SameSign_x,data_SameSign_y,data_SameSign_massess = format_data(missingOneHit_data_SameSign,allTrainingFeatures)\n",
    "  data_SameSign_weights = data_SameSign_x[:,data_SameSign_x.shape[1]-1]\n",
    "  data_SameSign_x = np.delete(data_SameSign_x,data_SameSign_x.shape[1]-1,1)            \n",
    "\n",
    "  SameSign_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    SameSign_pred.append(bdtArray[i].predict(data_SameSign_x))\n",
    "  \n",
    "  print(SameSign_pred)\n",
    "  counts_sameSign, binedges = np.histogram(SameSign_pred[1],bins=np.arange(0,1.01,0.04))\n",
    "  print(binedges)\n",
    "  print(counts_sameSign)'''\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84989374-71a3-4ba1-92bc-6b646cd26b28",
   "metadata": {},
   "source": [
    "BDT prediction of the ECAL Brem signal samples. Manipulates the dataframes in the same way the original 'fixB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a8d4f-3219-41a2-9314-8befbd7d2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "  sigPreds_ecalBrem = []\n",
    "  stacut = -0.6\n",
    "  filter = (allSignals_ecalBrem['standaloneDEoverE']<stacut) & (allSignals_ecalBrem['cellEdgeDeta']>0.004) & (allSignals_ecalBrem['cellEdgeDphi']>0.016)\n",
    "  filteredData = allSignals_ecalBrem[filter]\n",
    "  signal_ecalBrem_SystWeights = filteredData.copy()\n",
    "    \n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  filteredData.drop(['PUupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['PUdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['IDupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['IDdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['ISOupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['ISOdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['TrigUpWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['TrigDownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['EnBinWeight'],inplace=True, axis=1) \n",
    "  filteredData.drop(['EventWeight_cscReweight'],inplace=True, axis=1)     \n",
    "  print(filteredData)\n",
    "\n",
    "  signal_ecalBrem_SystWeights.drop(['pt'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['eta'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['phi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staDR'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staPhi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staE'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staChi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDR'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_0'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_1'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_2'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_3'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_4'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_5'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_6'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['probeCharge'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['standaloneDEoverE'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_0'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_1'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_2'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_3'],inplace=True, axis=1)        \n",
    "    \n",
    "  signal_ecalBrem_x,signal_ecalBrem_y,signal_ecalBrem_massess = format_data(filteredData,allTrainingFeatures)\n",
    "  signal_ecalBrem_weights = signal_ecalBrem_x[:,signal_ecalBrem_x.shape[1]-1]\n",
    "  signal_ecalBrem_x = np.delete(signal_ecalBrem_x,signal_ecalBrem_x.shape[1]-1,1)            \n",
    "\n",
    "  signal_ecalBrem_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    signal_ecalBrem_pred.append(bdtArray[i].predict(signal_ecalBrem_x))\n",
    "  \n",
    "  print(signal_ecalBrem_pred)\n",
    "\n",
    "  #Using predictions of testing dataset\n",
    "  sig0p2_ecalBrem_preds = []\n",
    "  sig0p4_ecalBrem_preds = []\n",
    "  sig0p6_ecalBrem_preds = []\n",
    "  sig0p8_ecalBrem_preds = []\n",
    "  sig1p0_ecalBrem_preds = []\n",
    "  for idx2, BDTscore in enumerate(signal_ecalBrem_pred[0]):\n",
    "            if signal_ecalBrem_massess[idx2] == \"0p2\": sig0p2_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p4\": sig0p4_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p6\": sig0p6_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p8\": sig0p8_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"1p0\": sig1p0_ecalBrem_preds.append(BDTscore)\n",
    "    \n",
    "  print(len(sig0p2_ecalBrem_preds))\n",
    "  print(len(sig0p4_ecalBrem_preds))\n",
    "  print(len(sig0p6_ecalBrem_preds))\n",
    "  print(len(sig0p8_ecalBrem_preds))\n",
    "  print(len(sig1p0_ecalBrem_preds))\n",
    "    \n",
    "  sigPreds_ecalBrem.append(sig0p4_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p8_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p6_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p2_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig1p0_ecalBrem_preds)    \n",
    " \n",
    "  #sigPreds_ecalBrem.append(sig0p4_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p6_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p2_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p8_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig1p0_ecalBrem_preds)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e00b8-9cf2-4cf9-8648-9db09edcdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "  '''stacut = -0.6\n",
    "  filter = (weightedMCInputs_highHCAL['standaloneDEoverE']<100) \n",
    "  filteredMC = weightedMCInputs_highHCAL[filter]    \n",
    "  filteredMC.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredMC.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_highHCAL = filteredMC.copy()\n",
    "\n",
    "  filter = (dataInputs_highHCAL['standaloneDEoverE']<100)\n",
    "  filteredData = dataInputs_highHCAL[filter]\n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHitData_highHCAL = filteredData.copy()\n",
    "\n",
    "  missingOneHit_highHCAL_x,missingOneHit_highHCAL_y,missingOneHit_highHCAL_massess = format_data(missingOneHit_highHCAL,allTrainingFeatures)\n",
    "  missingOneHit_highHCAL_weights = missingOneHit_highHCAL_x[:,missingOneHit_highHCAL_x.shape[1]-1]\n",
    "  missingOneHit_highHCAL_x = np.delete(missingOneHit_highHCAL_x,missingOneHit_highHCAL_x.shape[1]-1,1)            \n",
    "\n",
    "  missingOneHitData_highHCAL_x,missingOneHitData_highHCAL_y,missingOneHitData_highHCAL_massess = format_data(missingOneHitData_highHCAL,allTrainingFeatures)\n",
    "  missingOneHitData_highHCAL_weights = missingOneHitData_highHCAL_x[:,missingOneHitData_highHCAL_x.shape[1]-1]\n",
    "  missingOneHitData_highHCAL_x = np.delete(missingOneHitData_highHCAL_x,missingOneHitData_highHCAL_x.shape[1]-1,1)  \n",
    "    \n",
    "  MC_highHCAL_pred = []\n",
    "  Data_highHCAL_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    MC_highHCAL_pred.append(bdtArray[i].predict(missingOneHit_highHCAL_x))\n",
    "    Data_highHCAL_pred.append(bdtArray[i].predict(missingOneHitData_highHCAL_x))\n",
    "    \n",
    "  pd_MC_highHCAL_pred = pd.DataFrame({'BDTscore':MC_highHCAL_pred[0],'eventWeights':missingOneHit_highHCAL_weights}) \n",
    "\n",
    "  MC_highHCAL_Weights = pd_MC_highHCAL_pred['eventWeights']\n",
    "  MC_highHCAL_Weights_unity = MC_highHCAL_Weights/(MC_highHCAL_Weights.sum())\n",
    "  \n",
    "  pd_Data_highHCAL_pred = pd.DataFrame({'BDTscore':Data_highHCAL_pred[0]})\n",
    "#  pd_Data_highHCAL_pred.hist(column='BDTscore',log=True,\n",
    "#                             ax=axes,histtype='step',hatch='/', edgecolor=\"Red\",                             \n",
    "#                             bins=np.arange(0,1.01,0.04),label='Data (BDT 0)')\n",
    "\n",
    "  bin_edges = np.arange(0,1.04,0.05)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  #axes.errorbars(bin_centers, )\n",
    "        \n",
    "  \n",
    "  np_Data_highHCAL_pred = pd_Data_highHCAL_pred.to_numpy()\n",
    "  counts_data, binedges = np.histogram(np_Data_highHCAL_pred[:,0], bins=bin_edges)\n",
    "  np_counts_data = np.asarray(counts_data)\n",
    "  #counts_bkg_sqrt = np.sqrt(np_counts_bkg)/(testBkgWeights.sum()*0.04)\n",
    "    \n",
    "  counts_data_sqrt = []\n",
    "  totalCounts = 0\n",
    "  for counts in counts_data:\n",
    "    error = np.sqrt(counts)\n",
    "    counts_data_sqrt.append(error)\n",
    "    totalCounts = totalCounts + counts\n",
    "    \n",
    "  print(len(counts_data))\n",
    "  print(len(counts_data_sqrt))\n",
    "  print(counts_data_sqrt)\n",
    "\n",
    "  np_MC_highHCAL_pred = pd_MC_highHCAL_pred.to_numpy()\n",
    "  counts_MC, binedges = np.histogram(np_MC_highHCAL_pred[:,0], bins=bin_edges,weights=MC_highHCAL_Weights_unity*totalCounts)\n",
    "#  counts_MC, binedges = np.histogram(np_MC_highHCAL_pred[:,0], bins=bin_edges,weights=MC_highHCAL_Weights)\n",
    "  np_counts_MC = np.asarray(counts_MC)\n",
    "    \n",
    "  counts_MC_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < np_MC_highHCAL_pred[:,0], np_MC_highHCAL_pred[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = MC_highHCAL_Weights_unity[in_bin]*totalCounts\n",
    "#    weights_in_bin = MC_highHCAL_Weights[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_MC_sqrt.append(error)\n",
    "\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_MC, yerr=counts_MC_sqrt, fmt='s', ecolor='Blue',\n",
    "               mfc='Blue',mec='Blue',label='MC (BDT 0)')\n",
    "\n",
    "#  axes = pd_MC_highHCAL_pred.hist(column='BDTscore',log=True,\n",
    "#                                    bins=np.arange(0,1.04,0.05),\n",
    "#                                    weights=MC_highHCAL_Weights_unity*totalCounts,\n",
    "                                    #weights=pd_MC_highHCAL_pred['eventWeights']*59.8*398*1000*1.18*10**(-6),\n",
    "#                                    label='MC (BDT 0)')\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_data, yerr=counts_data_sqrt, fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Data (BDT 0)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"# of events\", loc='top')   \n",
    "  plt.legend(loc='upper center')#,ncol=2)\n",
    "  plt.grid(False)    \n",
    "  plt.yscale(\"log\")    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_highHCALE.pdf\")  '''  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a999a4-522d-405f-a7fc-b71f7d5738f2",
   "metadata": {},
   "source": [
    "Plot the ROC curves for each of the test BDTs. Multiplies the false and true positive rates by the standalone muon selection efficiency so that the curve is made for the whole dataset to allow for proper comparisons to the cut-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec3aa7-d464-41ba-8aa4-085297c615c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, curve in enumerate(curves):\n",
    "   print(idx)\n",
    "   print(sigMasses[idx])\n",
    "   fig = plt.figure(figsize = (8,8))\n",
    "   ax = fig.add_subplot(1,1,1)\n",
    "   ax.set_xlabel('False Positive Rate', fontsize = 15)\n",
    "   ax.set_ylabel('True Positive Rate', fontsize = 15)\n",
    "   ax.set_title('ROC Curve', fontsize = 20)\n",
    "#   ax.set_title('ROC Curve, '+sigMasses[idx], fontsize = 20)\n",
    "   fpr = curve[0]\n",
    "   tpr = curve[1]\n",
    "   fpralt = curve[2]*curve[4]\n",
    "   tpralt = curve[3]*curve[5]\n",
    "   fpralt_noC = curve[6]*curve[8]\n",
    "   tpralt_noC = curve[7]*curve[9]\n",
    "   xvar = fpr*curve[4]\n",
    "   yvar = tpr*curve[5]\n",
    "   #ax.plot(xvar, yvar, '-o',linewidth=3,markersize=0) \n",
    "   ax.plot(fpralt,tpralt, '-go',linewidth=3,markersize=0)\n",
    "#   ax.plot(fpralt_noC,tpralt_noC, '-ro',linewidth=3,markersize=0)\n",
    "   print(\"truePos\")\n",
    "   print(truePos)\n",
    "   ax.plot(falsePos,truePos[idx],'ko')\n",
    "   ax.grid()\n",
    "   ax.set_xscale('log')\n",
    "   ax.set_yscale('log')    \n",
    "#   ax.legend(['BDT','BDT (no track charge)','Cut Based Estimate'])\n",
    "#   ax.legend(['Scale POS Weight '+str(pos_weight),'Cut Based Estimate'])\n",
    "#   ax.legend(['Logistic Regression','Scale POS Weight '+str(pos_weight),'No HE Information','Cut Based Estimate'])\n",
    "   plt.xlim(0.000003,0.01)\n",
    "   fig.savefig(\"partialRegionBDTRocCurve.pdf\")\n",
    "\n",
    "   ax = importance[idx]\n",
    "   fig = ax.figure\n",
    "   fig.tight_layout()\n",
    "   fig.savefig(\"partialBDTImportance.pdf\")\n",
    "\n",
    "   #ax1 = importance[idx+1]\n",
    "   #fig1 = ax1.figure\n",
    "   #fig1.tight_layout()\n",
    "   #fig1.savefig(\"partialBDTImportance_noTrackCharge.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de12a6-69ae-4657-9598-4f3f4d7acd67",
   "metadata": {},
   "source": [
    "This part makes the dictionaries for my combine input maker. I had trouble making root files with jupyter lab (the python versions are incompatible, so I'd need to switch to a CERN made version of jupyter) so this writes the BDT prediction for each DY and signal point along with the corresponding weights directly to a pickle file, which I load in and then iterate over to fill a histogram in my combine input producer. Maybe more efficient to just used the saved BDTs to re-make the predictions in the other script, but as the file sizes aren't too big and I already have the predictions I did it this way. I also considered just saving the bin edges and counts, but getting the uncertainties correct is a bit of a pain that way.\n",
    "\n",
    "The file is structured as a list of dictionaries. The first six dictionaries each correspond to a signal mass, and have the mass, standalone muon selection efficiency, DY predictions, signal predictions, and systematic variations for the signal events at that mass.\n",
    "The final dictionary contains the event weights for the DY samples with each systematic variation, mainly so that the same information isn't repeated in every signal dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ad06d-0585-4961-911e-2d0bae9e17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDicts=[]\n",
    "DY_BDToutput_systWeights = BDToutput_systWeights[BDToutput_systWeights['label']<0.5]\n",
    "DY_BDToutput_systWeights.drop('label', inplace=True, axis=1)\n",
    "DY_BDToutput_systWeights.drop('massPoint', inplace=True, axis=1)\n",
    "print(len(newGenPreds[0]))\n",
    "print(len(sigPreds))\n",
    "print(len(curves))\n",
    "print(len(DY_BDToutput_systWeights))\n",
    "for idx, mass in enumerate(sigMasses):\n",
    "    print(idx)\n",
    "    print(mass)\n",
    "    sigSplitSystWeights = BDToutput_systWeights[BDToutput_systWeights['massPoint']==mass]\n",
    "    sigSplitSystWeights.drop(['label'], inplace=True, axis=1)\n",
    "    sigSplitSystWeights_ecalBrem = signal_ecalBrem_SystWeights[signal_ecalBrem_SystWeights['massPoint']==mass]\n",
    "    sigSplitSystWeights_ecalBrem.drop(['label'], inplace=True, axis=1)\n",
    "    massDict = {}\n",
    "    massDict['name']=mass\n",
    "#    massDict['dyEff']=curves[0][8]\n",
    "#    massDict['sigEff']=curves[0][9]\n",
    "#    massDict['mcPreds']=newGenPreds[1]\n",
    "#    massDict['sigPreds']=sigPreds[idx+6]\n",
    "    massDict['dyEff']=curves[0][4]\n",
    "    massDict['sigEff']=curves[0][5]    \n",
    "    massDict['mcPreds']=newGenPreds[0]\n",
    "    massDict['sigPreds']=sigPreds[idx]\n",
    "    #massDict['muPXPreds']=SameSign_pred[0]\n",
    "    print(len(sigPreds_ecalBrem[idx]))\n",
    "    massDict['sigPreds_ecalBrem']=sigPreds_ecalBrem[idx]    \n",
    "    print(\"len(sigPreds[idx])\")\n",
    "    print(len(sigPreds[idx]))\n",
    "    print(len(sigSplitSystWeights))\n",
    "    print(len(sigPreds_ecalBrem[idx]))\n",
    "    print(len(sigSplitSystWeights_ecalBrem))\n",
    "    for name, Systweights in sigSplitSystWeights.iteritems():\n",
    "        massDict[name]=Systweights.values\n",
    "    for name, Systweights in sigSplitSystWeights_ecalBrem.iteritems():\n",
    "        massDict[name+'_ecalBrem']=Systweights.values        \n",
    "    outputDicts.append(massDict)        \n",
    "dyWeightsDict={}\n",
    "for name,Systweights in DY_BDToutput_systWeights.iteritems():\n",
    "    dyWeightsDict[name]=Systweights.values\n",
    "print(len(dyWeightsDict))\n",
    "outputDicts.append(dyWeightsDict)\n",
    "pickle.dump(outputDicts, open('bdtPredictions'+sigMasses[4]+'.pkl', 'wb'),protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee3123-e4bd-4474-9a81-1e11dac8b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_features = ['disStaDEoverE', 'disStadEta', 'disStaChi2', 'refitStadEta', 'refitStaChi2', 'refitStaDEoverE', 'disStadPhi', 'refitStadPhi', 'refitStaE']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
